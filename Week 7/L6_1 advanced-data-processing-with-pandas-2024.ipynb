{"cells":[{"cell_type":"markdown","metadata":{"id":"t98eTC_RdULc"},"source":["#Lesson 6: Processing data with pandas II\n","This week we will continue developing our skills using [pandas](https://pandas.pydata.org/) to process real data.\n","\n","---\n","\n","General information\n","Sources\n","This lesson is inspired by the Geo-python module at the University of Helsinki which in turn acknowledges the Programming in Python lessons from the Software Carpentry organization. This version was adapted for Colab and a UK context by Ruth Hamilton.\n","\n","About this document\n","This is a Google Colab Notebook. This particular notebook is designed to introduce you to a few of the basic concepts of programming in Python. Like other common notebook formats (e.g. Jupyter), the contents of this document are divided into cells, which can contain:\n","\n","Markdown-formatted text,\n","Python code, or\n","raw text\n","You can execute a snippet of code in a cell by pressing Shift-Enter or by pressing the Run Cell button that appears when your cursor is on the cell .\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","source":["## Motivation\n","\n","\n","\n","According to the Met Office, 2020 was the [second warmest year in their record](https://www.metoffice.gov.uk/about-us/press-office/news/weather-and-climate/2021/2020-ends-earths-warmest-10-years-on-record). In this lesson, we will use our data manipulation and analysis skills to continue to analyze the weather data for 2020 in Sheffield.\n","\n","Along the way we will cover a number of useful techniques in pandas including:\n","\n","* renaming columns\n","* iterating data frame rows and applying functions\n","* data aggregation\n","* repeating the analysis task for several input files"],"metadata":{"id":"EYxlwfwEoVXk"}},{"cell_type":"markdown","metadata":{"id":"dsEEDz9adULl"},"source":["\n","\n","## About the data\n","\n","We will be working with the same data as last week but will be using the raw file as downloaded from the [CEDA archives](https://data.ceda.ac.uk/badc/ukmo-midas-open/data/uk-daily-temperature-obs/dataset-version-202207/south-yorkshire/00525_sheffield/qc-version-1.). I have downloaded the files but have kept the original file names which is a little more complicated.\n","\n","- We have data for Sheffield weather recordings for 1920, 1930, 1940, ...,2020.\n","- each file will have 91 rows of metadata at the start\n","\n","\n","We will develop our analysis workflow using data for a single year. Then, we will repeat the same process for all the years."]},{"cell_type":"markdown","metadata":{"id":"r__TjbX3dULm"},"source":["## Reading the data\n","\n","In order to get started, first mount your Google drive so we can access the data.let's first import pandas:"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"7xnE6VlSjH56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then import pandas."],"metadata":{"id":"G4rF932njMDB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"svzFO03idULn"},"outputs":[],"source":["#import pandas as pd\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"hD3m-Wq4dULo"},"source":["At this point, let's have another look at the dataset we used last week for 2020 and how it is structured. We are using a true CSV (comma separated value) file, i.e. the values are separated by commas (you can check this by opening one of the data files in a text editor like Notepad). This is the default for the `read_csv()` function but it can also read in data that uses another character (or characters) to separate values.\n","\n","---\n",">**Input data structure**\n",">\n",">- **Delimiter:** If your data uses a different *delimiter* , e.g. a white space, you can use specify either the `sep` parameter or the `delim_whitespace` paramter (see the documentation for the [read_csv() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html))\n","but note that you can't use both at the same time. By default, the `read_csv()` method assumes that the `sep` paramter is set to `,`, i.e. `sep=,`.\n",">\n",">- **No Data values:** Some datasets may use a character to indicate 'no_data'. We can tell pandas to consider those characters as NaNs by specifying `na_values=['*', '#']`. This would replace any value of `*` or `#` as a `NaN` value.\n","---"]},{"cell_type":"markdown","source":["Remember, each of the files starts with some metadata (90 rows in the raw data) so we need to add the `skiprows` paramtere when we read in the files."],"metadata":{"id":"FHI9lrlgp83u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9aVgcWtdULp"},"outputs":[],"source":["# Define relative path to the file\n","fp = r'/content/drive/Shareddrives/TRP479_Spatial_Data_science_2024/L6/Data/midas-open_uk-daily-temperature-obs_dv-202207_south-yorkshire_00525_sheffield_qcv-1_2020.csv'\n","\n","# Read data using varying amount of spaces as separator and specifying * characters as NoData values\n","data = pd.read_csv(fp, sep=',',skiprows=90)"]},{"cell_type":"markdown","metadata":{"id":"fCBAfu27dULq"},"source":["Let's see how the data looks by printing the first five rows with the `head()` function:"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"VNmLA9kidULr"},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","metadata":{"id":"UPhoxF3hdULr"},"source":["All seems ok. However, we won't be needing all of the 22 columns. We can check all column names by running `data.columns`.\n","> Remember, `columns` is one of the pandas *attributes* that you can use with a pandas dataframe. *attributes* are called *without* brackets, e.g. `data.columns` ."]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"HLcbGT0VdULr"},"outputs":[],"source":["#show the list of columns in the dataframe:\n","data.columns\n"]},{"cell_type":"markdown","source":["There are a number of columns that are probably not relevant, and also a lot that only contain `NaN` values. To get a better idea of what is contained in the metadata, we could look at the .csv file in Excel - or we could read it into a new dataframe in pandas.\n","\n","From last week, we know that the metadate for each file is contained in the first 90 rows. We can use the `nrows` paramter with the `read_csv()` method to specify the number of rows containing metadata (in this case, 90). We are also going to set the `header` paramter to `None` becasue there first line does not contain column headings."],"metadata":{"id":"GBqL3lX4tPVh"}},{"cell_type":"code","source":["#read the first 90 rows as meta-data\n","meta_data=pd.read_csv(fp,header=None,nrows=90)\n","\n"],"metadata":{"id":"IkWps6o1qb4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meta_data.head()"],"metadata":{"id":"Vcuphk66riBP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2NQQMCHdULr"},"source":["A description for all these columns is also available in the metadata file [midas-open_uk-daily-temperature-obs_dv-202207_station-metadata.csv](https://drive.google.com/open?id=1rRzYF4BGDxQWcO7tsbyiOS6l12ODg0Zv&authuser=r.hamilton%40sheffield.ac.uk&usp=drive_fs).\n","\n","### Reading in the data once again\n","\n","This time, we will read in only some of the columns using the `usecols` parameter. Let's read in columns that might be somehow useful to our analysis, or at least that contain some values that are meaningful to us, including the observation time, some id details, and the maximum and minimum air temperature readings: `'ob_end_time', 'id_type', 'ob_hour_count',\n","       'src_id',  'max_air_temp',\n","       'min_air_temp'`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-nsr7M0dULr"},"outputs":[],"source":["# Read in only selected columns\n","data = pd.read_csv(fp,\n","                   usecols=['ob_end_time', 'id_type', 'ob_hour_count',\n","       'src_id',  'max_air_temp',\n","       'min_air_temp'], skiprows=90)\n","\n","# Check the dataframe\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"oCPOJcSGdULs"},"source":["Okay, so we can see that the data was successfully read to the DataFrame."]},{"cell_type":"markdown","metadata":{"id":"NChlARBDdULs"},"source":["## Renaming columns\n","\n","As we saw above some of the column names are a bit awkward and difficult to interpret. Luckily, it is easy to alter labels in a pandas DataFrame using the [rename](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) function. In order to change the column names, we need to tell pandas how we want to rename the columns using a dictionary that lists old and new column names\n","\n","Let's first check again the current column names in our DataFrame:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDn_4foSdULs"},"outputs":[],"source":["data.columns"]},{"cell_type":"markdown","metadata":{"id":"kMJobPNedULs"},"source":["<div class=\"alert alert-info\"><b>Dictionaries</b><br/>\n","\n","A [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) is a specific data structure in Python for storing key-value pairs. During this course, we will use dictionaries mainly when renaming columns in a pandas series, but dictionaries are useful for many different purposes! For more information about Python dictionaries, check out [this tutorial](https://realpython.com/python-dicts/).\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"RdoTPu9ydULs"},"source":["We can define the new column names using a [dictionary](https://www.tutorialspoint.com/python/python_dictionary.htm) where we list \"`key: value`\" pairs, in which the original column name (the one which will be replaced) is the key and the new column name is the value.\n","\n","- Let's change the following:\n","   \n","   - `ob_end_time` to `TIMESTAMP`\n","   - `max_air_temp` to `MAX`\n","   - `min_air_temp` to `MIN`"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"JmtTEPf7dULt"},"outputs":[],"source":["# Create the dictionary with old and new names\n","new_names = {\"ob_end_time\" : \"TIMESTAMP\", \"max_air_temp\" :\"MAX\", \"min_air_temp\": \"MIN\"}\n","\n","# Let's see what the variable new_names look like\n","new_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lsoOejhdULt"},"outputs":[],"source":["# Check the data type of the new_names variable\n","type(new_names)"]},{"cell_type":"markdown","metadata":{"id":"yzVMG_WndULt"},"source":["From above we can see that we have successfully created a new dictionary.\n","\n","Now we can change the column names by passing that dictionary using the parameter `columns` in the `rename()` function:"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"P8m74Yc7dULt"},"outputs":[],"source":["# Rename the columns\n","data = data.rename(columns=new_names)\n","\n","# Print the new columns\n","print(data.columns)"]},{"cell_type":"markdown","metadata":{"id":"UwpVvlHDdULt"},"source":["Perfect, now our column names are easier to understand and use."]},{"cell_type":"markdown","metadata":{"id":"TstP7AMUdULu"},"source":["### Check your understanding\n","\n","The `src_id` columns contains the *id* code for the location, let's rename the column `src_id` to `source_id`."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["hide-cell"],"id":"5PJoMwDfdULu"},"outputs":[],"source":["# Create the dictionary with old and new names\n","\n","\n","# Rename the columns\n","\n","\n","# Check the output\n","data.head()"]},{"cell_type":"code","source":["#@title Click to show code\n","# Create the dictionary with old and new names\n","new_names={\"src_id\":\"source_id\"}\n","\n","# Rename the columns\n","data = data.rename(columns=new_names)\n","\n","# Check the output\n","data.head()"],"metadata":{"cellView":"form","id":"GQZWy8to1PnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XW6LkAMdULu"},"source":["## Data properties\n","\n","As we learned last week, it's always a good idea to check basic properties of the input data before proceeding with the data analysis. Let's check the:\n","\n","- Number of rows and columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5G9x-xedULu"},"outputs":[],"source":["data.shape"]},{"cell_type":"markdown","metadata":{"id":"bGVoyJWndULu"},"source":["- Top and bottom rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28pM3uIgdULu"},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9NyEZRKdULu"},"outputs":[],"source":["data.tail()"]},{"cell_type":"markdown","metadata":{"id":"jLkvaKyydULv"},"source":["- Data types of the columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1lBwdZLdULv"},"outputs":[],"source":["data.dtypes"]},{"cell_type":"markdown","metadata":{"id":"Q92mOWlFdULv"},"source":["- Descriptive statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"T-t3zT5BdULv"},"outputs":[],"source":["data.describe()"]},{"cell_type":"markdown","source":["You should have noticed that the last line of our `data.tail()` call conatined the `end data` flag. We need to remove that line again..."],"metadata":{"id":"o_cMG5tN3UpL"}},{"cell_type":"code","source":["# 'drop' the last row of data in our data frame\n","data.drop(len(data)-1,axis = 'index',inplace =True)"],"metadata":{"id":"O-Gcf5t11-OY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SWNNZoO5dULz"},"source":["## Parsing dates\n","\n","You will have noticed that although the newly named `TIMESTAMP` column contains time/data information, the data type is read as an *object*. We will eventually want to group our data based on month . We also want to be able to distinguish between *daytime* and *nighttime* readings (those recorded in the 12 hours preceding 21:00, and those recorded in the 12 hours before 09:00, respectively)."]},{"cell_type":"markdown","metadata":{"id":"4iezT410dULz"},"source":["Let's have a closer look at the date and time information we have by checking the values in that column, and their data type:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FpqbYshdULz"},"outputs":[],"source":["data[\"TIMESTAMP\"].head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HiokZOY9dULz"},"outputs":[],"source":["data[\"TIMESTAMP\"].tail(10)"]},{"cell_type":"markdown","metadata":{"id":"bAyVW7SgdULz"},"source":["The `TIMESTAMP` column contains two observations per day. The timestamp for the first observation is `2020-01-01 09:00:00`, i.e. from 1st of January 2020, and the timestamp for the latest observation is `2020-12-31 21:00:00`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3sH0nM-idULz"},"outputs":[],"source":["#check the data types again\n","data.dtypes"]},{"cell_type":"markdown","metadata":{"id":"dOPH31TTdUL0"},"source":["The date information in TIMESTAMP is stored as an *object*, i.e. a *string*.\n","\n","We want to extract the day and night temperatures and **aggregate the data on a monthly level**.  In order to do so, we need to \"label\" each row of data based on wether it is day or night and the month when the record was observed. In order to do this, we need to somehow separate information about the month and time for each row.\n","\n","We create these \"labels\" by making a new column (or an index) containing information about the month and the time.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LduBCDshdUL0"},"source":["### String slicing\n","\n","Our data is in string format, which can actually make it quite easy to work with. Our next step is to \"cut\" the needed information from the [string objects](https://docs.python.org/3/tutorial/introduction.html#strings). If we look at the latest time stamp in the data (`2020-01-01 09:00:00`), you can see that there is a systematic pattern `YEAR-MONTH-DAY HOUR:MINUTE:SECONDS`. The year, month and day are separated by a `-` , then there is a white space followed by the time using the 24 hour clock and `hours:minutes:seconds`!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QT5JiGaSdUL0"},"outputs":[],"source":["#let's look at the first entry in the TIME column:\n","day_1=data.iloc[0,0]\n","\n","#print it on the screen\n","print(day_1)\n","\n","#identify the part of the string with the *data* information\n","print(day_1[0:10])"]},{"cell_type":"markdown","metadata":{"id":"-Kh_3rcvdUL0"},"source":["Based on this information, we can **slice** the correct range of characters from the `TIME` column using [pandas.Series.str.slice()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.slice.html)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N251IL3odUL0"},"outputs":[],"source":["# Slice the string\n","data[\"DATE\"] = data[\"TIMESTAMP\"].str.slice(start=0, stop=10)\n","\n","# Let's see what we have\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"0pdXil5kdUL0"},"source":["Nice! Now we have added an attribute to the rows based on information about date."]},{"cell_type":"markdown","metadata":{"id":"9Kotumw0dUL0"},"source":["### Check your understanding\n","\n","Create three new columns, a `'MONTH'` column with only the **month**, `'DAY'` column with only the **day**, and a `'TIME'` column with information about the **time**."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["hide-cell"],"id":"Cm0rMOPRdUL1"},"outputs":[],"source":["# Extract information about *time* from the TIMESTAMP column into a new column 'TIME':\n"]},{"cell_type":"code","source":["# Extract information about *day* from the TIMESTAMP column into a new column 'DAY':"],"metadata":{"id":"nWCuq1LXhPea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract information about *month* from the TIMESTAMP column into a new column 'MONTH':"],"metadata":{"id":"pYP_n5NchPPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Click to show code\n","# Extract information about *month* from the TIMESTAMP column into a new column 'MONTH':\n","data[\"MONTH\"] = data[\"TIMESTAMP\"].str.slice(start=5, stop=7)\n","\n","# Extract information about *day* from the TIMESTAMP column into a new column 'DAY':\n","data[\"DAY\"] = data[\"TIMESTAMP\"].str.slice(start=8, stop=10)\n","\n","# Extract information about *time* from the TIMESTAMP column into a new column 'TIME':\n","# Slice the string\n","data[\"TIME\"] = data[\"TIMESTAMP\"].str.slice(start=11)\n","\n"],"metadata":{"cellView":"form","id":"El_ZtJHf_ozF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPL18lcrdUL1"},"outputs":[],"source":["# Check the result\n","data.head()"]},{"cell_type":"markdown","source":["Finally, let's create an identifier that contains the 'month and day' information; this allows us to easily identify records for a 24 hour period."],"metadata":{"id":"iquIC73etqX4"}},{"cell_type":"code","source":["data.dtypes\n"],"metadata":{"id":"dDaR7rRCuE1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['MONTH_DAY']=data['MONTH'] + data['DAY']\n","data.head()"],"metadata":{"id":"KGHKSRoDtw27"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this section, we have extracted date, month, day and time information from a text formatted string - **this is a very useful skill!** - and one that is very commonly used in Data Science. These methods will work on any strings e.g. postcodes, email addresses etc, although they may get a little more complicated if the text is not formatted as simply as it is here; this is where your problem-solving skills will come in handy!\n","\n","Next, we will look at how pandas can work *natively* with date/time information."],"metadata":{"id":"fP2SlFDHCcaP"}},{"cell_type":"markdown","metadata":{"id":"iBLkiQyidUL1"},"source":["### Datetime\n","\n","In pandas, we can convert dates and times into a new data type [datetime](https://docs.python.org/3.7/library/datetime.html) using [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJ9BNWITdUL1"},"outputs":[],"source":["# Convert character strings to datetime\n","data['DATE_dt'] = pd.to_datetime(data['DATE'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0C6KBB0edUL1"},"outputs":[],"source":["# Check the output\n","data['DATE_dt'].head()"]},{"cell_type":"markdown","metadata":{"id":"Bkqncy-zdUL1"},"source":["<div class=\"alert alert-info\"><b>Pandas Series datetime properties</b><br/>\n","\n","There are several methods available for accessing information about the properties of datetime values. Read more from the pandas documentation about [datetime properties](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetime-properties).\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"KFF4elGBdUL2"},"source":["Now, we can extract different time units based on the datetime-column using the [pandas.Series.dt](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.html) accessor:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RXFD2q0dUL2"},"outputs":[],"source":["data['DATE_dt'].dt.year"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyhX_9W-dUL2"},"outputs":[],"source":["data['DATE_dt'].dt.month"]},{"cell_type":"markdown","metadata":{"id":"8BM2AjpVdUL2"},"source":["We can also combine the datetime functionalities with other methods from pandas. For example, we can check the number of unique months in our input data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QbNJWn2xdUL2"},"outputs":[],"source":["data['DATE_dt'].dt.month.nunique()"]},{"cell_type":"markdown","source":[],"metadata":{"id":"urq4rZU_VWLg"}},{"cell_type":"markdown","metadata":{"id":"uc5-1YS_dUL3"},"source":["## Aggregating data in Pandas by grouping\n","\n","Here, we will learn how to use [pandas.DataFrame.groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) which is a handy method for compressing large amounts of data and computing statistics for subgroups.\n","\n","We will use the groupby method to calculate the average *day* and *night* temperatures for each month through these main steps:\n","\n","  1. **Grouping the data** based on the time of day and month\n","  2. Calculating the average for each month (each group)\n","  3. Storing those values into **a new DataFrame** called `monthly_data`"]},{"cell_type":"markdown","metadata":{"id":"NCyMqn5UdUL3"},"source":["Before we start grouping the data, let's once more see what our input data looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4uhqbEjkdUL3"},"outputs":[],"source":["print(\"number of rows:\", len(data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q76G6LvUdUL3"},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","metadata":{"id":"YigW_SkkdUL3"},"source":["We have quite a few rows of weather data, and two observations per day. Our goal is to create an aggreated data frame that would have only one row per month with day and nighttime temperatures."]},{"cell_type":"markdown","source":["Our `TIME` attribute is a bit confusing (09:00:00 corresponds the *night time* reading and 21:00:00 corresponds to the *day time* reading). Let's create a column, `TofDay`, that contains a text flag indicating night or day time readings.\n","\n","We are going to use the `.loc` method we used last week to up date the values in our new `TofDay` column depending on the value in the `TIME` column."],"metadata":{"id":"_sAGBFJRVpiW"}},{"cell_type":"code","source":["\n","data.loc[data['TIME'] == \"21:00:00\",'TofDay'] = 'Day'\n","data.loc[data['TIME'] == \"09:00:00\",'TofDay'] = 'Night'"],"metadata":{"id":"Tmi-K92CTp4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.head()"],"metadata":{"id":"7ba_Tq94Umuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's also create an *average* day/nightime temperature for each day. We will assume this is simply the *mean* of the minimum and maximum air temeratures for each 12 hour period. To reduce confusion, we will call this `'MID'` to indicate that it is the *midpoint* between the `MAX` and `MIN` values."],"metadata":{"id":"bGqq64OieI2D"}},{"cell_type":"code","source":["data['MID']=(data['MAX']+data['MIN'])/2\n","data.head()"],"metadata":{"id":"ewpe--Xpeccu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gddkJy0UdUL3"},"source":["Now we can start grouping our data. Let's **group** our data based on the  month."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YdyvNVEadUL4"},"outputs":[],"source":["grouped_m = data.groupby(\"MONTH\")"]},{"cell_type":"markdown","metadata":{"id":"7OH63Xm-dUL4"},"source":["---\n",">**Note**\n",">\n",">It is also possible to create combinations on-the-fly when grouping the data:\n","  >\n",">```  \n","grouped_mt = data.groupby(['MONTH', 'TofDay'])\n","```\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Wb5TqkPZdUL4"},"source":["Let's explore the new variable `grouped_m`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFe_ZQZkdUL4"},"outputs":[],"source":["type(grouped_m)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLfOwOwBdUL4"},"outputs":[],"source":["len(grouped_m)"]},{"cell_type":"markdown","metadata":{"id":"MCdVJcbIdUL4"},"source":["We have a new object with type `DataFrameGroupBy` with 12 groups. In order to understand what just happened, let's also check the number of unique month  combinations in our data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VOa4maEjdUL4"},"outputs":[],"source":["data['MONTH'].nunique()"]},{"cell_type":"markdown","metadata":{"id":"hj99tDZDdUL4"},"source":["Length of the grouped object should be the same as the number of unique values in the column we used for grouping. For each unique value, there is a group of data."]},{"cell_type":"markdown","metadata":{"id":"ExsbE8audUL5"},"source":["Let's explore our grouped data even further.\n","\n","We can check the \"names\" of each group."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phPAgbnYdUL5"},"outputs":[],"source":["# Next line will print out all 12 group \"keys\"\n","grouped_m.groups.keys()"]},{"cell_type":"markdown","metadata":{"id":"5URIefPZdUL5"},"source":["### Accessing data for one group\n","\n","Let us now check the contents for the group representing June (the name of that group is `06`). We can get the values of that hour from the grouped object using the `get_group()` method."]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"R_iqaMmBdUL5"},"outputs":[],"source":["# Specify a month (as character string)\n","month=\"06\"\n","\n","# Select the group\n","group1 = grouped_m.get_group(month)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nw59wWdIdUL5"},"outputs":[],"source":["# Let's see what we have\n","group1.head()\n"]},{"cell_type":"markdown","metadata":{"id":"k6CQZVgedUL5"},"source":["Ahaa! As we can see, a single group contains a **DataFrame** with values only for that specific month. Let's check the DataType of this group."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YC64EQ26dUL5"},"outputs":[],"source":["type(group1)\n"]},{"cell_type":"markdown","metadata":{"id":"Y2wLmmDUdUL5"},"source":["So, as noted above, one group is a pandas DataFrame! This is really useful, because we can now use all the familiar DataFrame methods for calculating statistics, etc. for this specific group. We can, for example, calculate the average values for all variables using the statistical functions that we have seen already (e.g. mean, std, min, max, median, etc.).\n","\n","We can do that by using the `mean()` function that we already did during Lesson 5.\n","\n","- Let's calculate the mean for following attributes all at once:\n","\n","    - `MAX`\n","    -`MIN`\n","    -`MID`"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"GU9l3LkJdUL6"},"outputs":[],"source":["# Specify the columns that will be part of the calculation\n","mean_cols = ['MAX','MIN','MID']\n","\n","# Calculate the mean values all at one go\n","mean_values = group1[mean_cols].mean()\n","\n","# Let's see what we have\n","print(mean_values)"]},{"cell_type":"markdown","metadata":{"id":"lIAx_hIRdUL6"},"source":["Above, we saw how you can access data from a single group. In order to get information about all groups (all months) we can use a `for` loop or methods available in the grouped object.\n","\n","### For loops and grouped objects\n","\n","When iterating over the groups in our `DataFrameGroupBy` object it is important to understand that a single group in our `DataFrameGroupBy` actually contains not only the actual values, but also information about the `key` that was used to do the grouping. Hence, when iterating over the data we need to assign the `key` and the values into separate variables.\n","\n","So, let's see how we can iterate over the groups and print the key and the data from a single group (again using `break` to only see what is happening for the first group)."]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"qV8lrqBkdUL6"},"outputs":[],"source":["# Iterate over groups\n","for key, group in grouped_m:\n","    # Print key and group\n","    print(f\"Key:\\n {key}\")\n","    print(f\"\\nFirst rows of data in this group:\\n {group.head()}\")\n","\n","    # Stop iteration with break command; we are using this to stop the loop in the first iteration so we can see what is going on.\n","    break"]},{"cell_type":"markdown","metadata":{"id":"ig1VUpHudUL6"},"source":["OK, so from here we can see that the `key` contains the 'name' of the group (month).\n","\n","Let's build on this and see how we can create a DataFrame where we calculate the mean values for all those weather attributes that we were interested in. We will repeat some of the earlier steps here so you can see and better understand what is happening."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDrWsDbEdUL6"},"outputs":[],"source":["# Create an empty DataFrame for the aggregated values\n","monthly_data = pd.DataFrame()\n","\n","# The columns that we want to aggregate\n","mean_cols = [\"MAX\", \"MIN\" , \"MID\"]\n","\n","# Iterate over the groups\n","for key, group in grouped_m:\n","\n","    # Calculate mean - this is the same code we used in Cell 53...\n","    mean_values = group[mean_cols].mean()\n","\n","    # Add the ´key´ (i.e. the date+time information) to the mean values\n","    mean_values[\"MONTH\"] = key\n","\n","    # Convert the mean_values series to a DataFrame and make it have a row orientation\n","    row = mean_values.to_frame().transpose()\n","\n","    # Concatenate the aggregated values into the monthly_data DataFrame\n","    monthly_data = pd.concat([monthly_data, row], ignore_index=True)"]},{"cell_type":"markdown","source":["This has calculated the *mean* value for each month."],"metadata":{"id":"IjlCBIuuXo88"}},{"cell_type":"code","source":["mean_values"],"metadata":{"id":"Rw_7hGRC1XpC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BqYJa2xsdUL6"},"source":["Now, let us see what we have."]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"G4WSRTapdUL7"},"outputs":[],"source":["print(monthly_data)"]},{"cell_type":"markdown","metadata":{"id":"HQBbfe7AdUL9"},"source":["Awesome! Now we have aggregated our data and we have a new DataFrame called `monthly_data` where we have mean values for each month in the data set."]},{"cell_type":"markdown","metadata":{"id":"FCYNSUVbdUL-"},"source":["### Finding the mean for all groups at once\n","\n","We can also achieve the same result by computing the mean of all columns for all groups in the grouped object."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"du5EmldDdUL-"},"outputs":[],"source":["grouped_m.mean()"]},{"cell_type":"code","source":["grouped_m.mean(numeric_only=True)"],"metadata":{"id":"2WApQvO_c4RC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also link the methods together. The lines below show how we can group on mutliple columns and calculate the mean all at once."],"metadata":{"id":"TjMuWXIuc47Y"}},{"cell_type":"code","source":["data_tm= data.groupby(['TofDay','MONTH'])['MAX','MIN','MID'].mean()\n","print(\"Grouped by time of day *then* month:\\n\",data_tm.head()) # note the use of the 'newline' charcater '\\n'.\n","\n","data_mt=data.groupby(['MONTH','TofDay'])[['MAX','MIN','MID']].mean()\n","print(\"\\n Grouped by month *then* time of day:\\n\",data_mt.head())"],"metadata":{"id":"On8MjXy6by0L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **NOTE** the first line `data_tm= data.groupby(['TofDay','MONTH'])['MAX','MIN','MID'].mean()` resulted in a **warning**. After a quick internet search, I found the solution is to put the columns into a **list** (as indicated by the `[[  ]]` in the second line).\n",">\n",">Programming standards can change quickly and you will encounter warnings and syntax errors frequently. With warnings, the code will execute *for now* but it is a good idea to follow up on them so that you can prevent your code from breaking in the future."],"metadata":{"id":"Rfswpz733EMT"}},{"cell_type":"markdown","source":["Although we have used the `groupby` method, the result of the `mean()` method is a dataframe. We can check that by checking the `type`. Since it is a dataframe, we can use `.loc` and `.iloc` functionality to extract individual values.\n","\n","\n"],"metadata":{"id":"ciMRzgSaiU73"}},{"cell_type":"code","source":["print(type(data_mt))\n","print(\"Average July night-time temperatures\\n\",data_mt.loc['07', 'Night'],\"\\n\",sep='')\n","print(\"Average July temperatures\\n\", data_mt.loc['07'],sep='')"],"metadata":{"id":"kmpBVveGhLCY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["group_md=data.groupby('MONTH_DAY')['MID'].mean()\n","group_md.head()\n","#group_md['mean_24']=group_md[['MID']].mean()\n","#df['average_1_3'] = df[['salary_1', 'salary_3']].mean(axis=1)\n","#group_md.head()"],"metadata":{"id":"GAVGh8IRzZPB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"anMVdh8AdUL-"},"source":["## Detecting warm months\n","\n","Now that we have aggregated our data on monthly level, all we need to do is to sort our results in order to check which month had the warmest daytime and nighttime temperatures. A simple approach is to select all Daytime readings from the data, group the data and check which group(s) have the highest mean value.\n","\n","We can start this by selecting all records that are from the Day (regardless of the month)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHGkBn6HdUL-"},"outputs":[],"source":["daytime = data[data[\"TofDay\"] == \"Day\"]"]},{"cell_type":"markdown","metadata":{"id":"24bCm3FMdUL_"},"source":["We can group by the month."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cV3klTZ3dUL_"},"outputs":[],"source":["grouped = daytime.groupby(by=\"MONTH\")"]},{"cell_type":"markdown","metadata":{"id":"Y7FAfettdUL_"},"source":["And then we can calculate the mean for each group."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgVDlNYFdUMA"},"outputs":[],"source":["monthly_day_mean = grouped.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdDJCuaWdUMA"},"outputs":[],"source":["monthly_day_mean.head()"]},{"cell_type":"markdown","metadata":{"id":"yVBYzisGdUMA"},"source":["Finally, we can sort and check the highest temperature values. We can sort the data frame in a descending order to do this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqV5fqy3dUMA"},"outputs":[],"source":["monthly_day_mean.sort_values(by=\"MID\", ascending=False)"]},{"cell_type":"markdown","metadata":{"id":"bvVZP50udUMA"},"source":["So, what month had the highest average daytime temperature?"]},{"cell_type":"markdown","source":["## Check your understanding\n","\n","Now find the month with the highest average nightime temperature"],"metadata":{"id":"2-4cfmxkkk9-"}},{"cell_type":"code","source":["#Now, find the month with the highest average nightime temperature"],"metadata":{"id":"AA2JTolDkuWg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Up until 2010, the data only recorded the maximum and minimum over a 24 hour\n","period. This means, if we want to make comparisons across multiple years, we will have to calculate an *average* for the 24 hour period. Again our simplest assumption is that this is simply the average of the max and minimum values from both day and night time readings.\n","\n","Calculating this is going to be a bit more complicated...\n","\n","We can use our *uniue* `MONTH_DAY` column to group all values for one day, and then find the *mean* of the `MID` column."],"metadata":{"id":"jbCZEelnr-yj"}},{"cell_type":"code","source":["# Group by month and day\n","grouped_month_day = data.groupby(by=['MONTH_DAY'])\n","daily_mean=grouped_month_day.mean()\n","\n","print(daily_mean.sort_values(by='MID', ascending=False).head(5))\n","print(\"\\n\")"],"metadata":{"id":"bLF4foHbsjz5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check that values for July 31st have mean of 23.375\n","print(data.loc[(data['MONTH']==\"07\") & (data['DAY']==\"31\")])        # shows all values for July 31\n","print(data.loc[(data['MONTH']==\"07\") & (data['DAY']==\"31\")].mean()) #shows mean for July 31 values\n","\n","\n"],"metadata":{"id":"7mSC441B5q8p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElES3crWdUMA"},"source":["## Repeating the data analysis with a larger dataset\n","\n","To wrap up today's lesson, let's repeat the data analysis steps above for all the available data we have (!!). First, it would be good to confirm the path to the **folder** where all the input data are located.\n","\n","The idea is, that we will repeat the analysis process for each input file using a (rather long) for loop! In the following cide block, we pull together all the main analysis steps we have done so far, along with some additional output info. If this works, we cen then look at integrating into a loop so we can automate the process over a number of files."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoxOEc1kdUMB"},"outputs":[],"source":["# this script brings together all of the main steps we have taken from reading in the data to renaming and creating columns\n","# Read in only selected columns\n","fp = r'/content/drive/Shareddrives/TRP479_Spatial_Data_science_2024/L6/Data/midas-open_uk-daily-temperature-obs_dv-202207_south-yorkshire_00525_sheffield_qcv-1_2020.csv'\n","\n","data = pd.read_csv(fp,\n","                   usecols=['ob_end_time', 'id_type', 'ob_hour_count',\n","       'src_id',  'max_air_temp',\n","       'min_air_temp'], skiprows=90)\n","\n","# 'drop' the last row of data in our data frame becasue we know it contains the 'end data' flag\n","data.drop(len(data)-1,axis = 'index',inplace =True)\n","\n","# Rename the columns\n","new_names = {\"ob_end_time\" : \"TIMESTAMP\", \"max_air_temp\" :\"MAX\", \"min_air_temp\": \"MIN\"}\n","data = data.rename(columns=new_names)\n","\n","\n","#Print info about the current input file:\n","print(\"Date:\", data.at[0,\"TIMESTAMP\"])\n","print(\"NUMBER OF OBSERVATIONS:\", len(data))\n","\n","# Create column\n","col_name = 'MID'\n","data[col_name] = None\n","\n","# Calculate 'average' temperature as midpoint between max and min readings\n","data['MID'] = (data['MAX'] + data['MIN'])/2\n","\n","\n","# Slice the string into DATE, MONTH, DAY and TIME columns\n","data[\"DATE\"] = data[\"TIMESTAMP\"].str.slice(start=0, stop=10)\n","data[\"MONTH\"] = data[\"TIMESTAMP\"].str.slice(start=5, stop=7)\n","data[\"DAY\"] = data[\"TIMESTAMP\"].str.slice(start=8, stop=10)\n","data[\"TIME\"] = data[\"TIMESTAMP\"].str.slice(start=11)\n","\n","#make MONTH_DAY unique identifier\n","data['MONTH_DAY']=data['MONTH']+data['DAY']\n","\n","#create 'Time of Day' flag\n","data.loc[data['TIME'] == \"21:00:00\",'TofDay'] = 'Day'\n","data.loc[data['TIME'] == \"09:00:00\",'TofDay'] = 'Night'\n","\n","\n","\n","# Extract observations for the months of days and nights\n","days = data[data['TofDay']==\"Day\"]\n","nights = data[data['TofDay']==\"Night\"]\n","\n","# Group by year and month\n","grouped_day = days.groupby(by=['MONTH'])\n","grouped_night = nights.groupby(by=['MONTH'])\n","\n","# Get mean values for each group\n","monthly_day_mean = grouped_day.mean()\n","monthly_night_mean = grouped_night.mean()\n","\n","# Print info\n","print(monthly_day_mean.sort_values(by='MID', ascending=False).head(5))\n","print(\"\\n\")"]},{"cell_type":"markdown","source":["###But\n","\n","Prior to 2010, only a single Min and Max reading was taking for each 24 hour period. So we need to calculate the mean for the 24 hour period *for all files*. We can do this using the `MONTH_DAY` flag."],"metadata":{"id":"uswfAKiJ8UU1"}},{"cell_type":"code","source":["# this script brings together all of the main steps we have taken from reading in the data to renaming and creating columns\n","# Read in only selected columns\n","fp = r'/content/drive/Shareddrives/TRP479_Spatial_Data_Science_Data/L6/Data/midas-open_uk-daily-temperature-obs_dv-202207_south-yorkshire_00525_sheffield_qcv-1_2020.csv'\n","\n","data = pd.read_csv(fp,\n","                   usecols=['ob_end_time', 'id_type', 'ob_hour_count',\n","       'src_id',  'max_air_temp',\n","       'min_air_temp'], skiprows=90)\n","\n","# 'drop' the last row of data in our data frame becasue we know it contains the 'end data' flag\n","data.drop(len(data)-1,axis = 'index',inplace =True)\n","\n","# Rename the columns\n","new_names = {\"ob_end_time\" : \"TIMESTAMP\", \"max_air_temp\" :\"MAX\", \"min_air_temp\": \"MIN\"}\n","data = data.rename(columns=new_names)\n","\n","\n","#Print info about the current input file:\n","print(\"Date:\", data.at[0,\"TIMESTAMP\"])\n","print(\"NUMBER OF OBSERVATIONS:\", len(data))\n","\n","# Create column\n","col_name = 'MID'\n","data[col_name] = None\n","\n","# Calculate 'average' temperature as midpoint between max and min readings\n","data['MID'] = (data['MAX'] + data['MIN'])/2\n","\n","\n","# Slice the string into DATE, MONTH, DAY and TIME columns\n","data[\"DATE\"] = data[\"TIMESTAMP\"].str.slice(start=0, stop=10)\n","data[\"MONTH\"] = data[\"TIMESTAMP\"].str.slice(start=5, stop=7)\n","data[\"DAY\"] = data[\"TIMESTAMP\"].str.slice(start=8, stop=10)\n","data[\"TIME\"] = data[\"TIMESTAMP\"].str.slice(start=11)\n","\n","#make MONTH_DAY unique identifier\n","data['MONTH_DAY']=data['MONTH']+data['DAY']\n","\n","#create 'Time of Day' flag\n","#data.loc[data['TIME'] == \"21:00:00\",'TofDay'] = 'Day'\n","#data.loc[data['TIME'] == \"09:00:00\",'TofDay'] = 'Night'\n","\n","# Group by month and day\n","grouped_month_day = data.groupby(by=['MONTH_DAY'])\n","daily_mean=grouped_month_day.mean()\n","\n","#group by month\n","grouped_month= data.groupby(by=['MONTH'])\n","monthly_mean=grouped_month.mean()\n","\n","# Print info\n","print(daily_mean.sort_values(by='MID', ascending=False).head(5))\n","print(monthly_mean.sort_values(by='MID', ascending=False).head(5))\n","print(\"\\n\")\n","\n","#group_md=data.groupby('MONTH_DAY')['MID'].mean()\n","#group_md.head()"],"metadata":{"id":"RjlsYaJr2N2b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2GYxYfbgdUMB"},"source":["At this point we will use the `glob()` function from the module `glob` to list all of our input files. `glob` is a handy function for finding files in a directrory that match a given pattern, for example."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggJ1iaH-dUMB"},"outputs":[],"source":["import glob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxIGgwZNdUMB"},"outputs":[],"source":["file_list = glob.glob(r\"/content/drive/Shareddrives/TRP479_Spatial_Data_Science_Data/L6/Data/midas-open_uk-daily-temperature-obs_dv-202207_south-yorkshire*csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"YhEh4x2-dUMC"},"source":["> **Note** that we're using the \\* character as a wildcard, so any file that starts with `/content/drive/Shareddrives/TRP479_Spatial_Data_Science_Data/L6/Data/midas-open_uk-daily-temperature-obs_dv-202207_south-yorkshire` and ends with `csv` will be added to the list of files we will iterate over. We specifically use the path up to `_south-yorkshire`  to avoid having our metadata file included in the list.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T941DsqTdUMC"},"outputs":[],"source":["print(\"Number of files in the list\", len(file_list))\n","print(file_list)"]},{"cell_type":"markdown","metadata":{"id":"Td4TQSIIdUMC"},"source":["Now, you should have all the relevant file names in a list, and we can loop over the list using a for loop."]},{"cell_type":"code","source":["#creates an empty pandas dataframe to store our yearly data\n","summary_data = pd.DataFrame([])\n","\n","# Repeat the analysis steps for each input file:\n","for fp in file_list:\n","\n","# Read in only selected columns\n","   data = pd.read_csv(fp,\n","                    usecols=['ob_end_time', 'id_type', 'ob_hour_count',\n","       'src_id',  'max_air_temp',\n","       'min_air_temp'], skiprows=90)\n","\n","# 'drop' the last row of data in our data frame becasue we know it contains the 'end data' flag\n","   data.drop(len(data)-1,axis = 'index',inplace =True)\n","\n","# Rename the columns\n","   new_names = {\"ob_end_time\" : \"TIMESTAMP\", \"max_air_temp\" :\"MAX\", \"min_air_temp\": \"MIN\"}\n","   data = data.rename(columns=new_names)\n","\n","\n","#Print info about the current input file:\n","   print(\"Date:\", data.at[0,\"TIMESTAMP\"])\n","   print(\"NUMBER OF OBSERVATIONS:\", len(data))\n","\n","# Create column\n","   col_name = 'MID'\n","   data[col_name] = None\n","\n","# Calculate 'average' temperature as midpoint between max and min readings\n","   data['MID'] = (data['MAX'] + data['MIN'])/2\n","\n","\n","# Slice the string into DATE, YEAR, MONTH, DAY and TIME columns\n","\n","   data[\"DATE\"] = data[\"TIMESTAMP\"].str.slice(start=0, stop=10)\n","   data[\"YEAR\"] =data[\"TIMESTAMP\"].str.slice(start=0, stop=4)\n","   data[\"MONTH\"] = data[\"TIMESTAMP\"].str.slice(start=5, stop=7)\n","   data[\"DAY\"] = data[\"TIMESTAMP\"].str.slice(start=8, stop=10)\n","   data[\"TIME\"] = data[\"TIMESTAMP\"].str.slice(start=11)\n","\n","#make MONTH_DAY unique identifier\n","   data['MONTH_DAY']=data['MONTH']+data['DAY']\n","\n","\n","# Group by day\n","   grouped_month_day = data.groupby(by=['MONTH_DAY'])\n","   daily_mean=grouped_month_day.mean(numeric_only=True)\n","   daily_mean['YEAR']=str(data.loc[0]['YEAR'])  #stores the YEAR\n","\n","   #group by month\n","   grouped_month= data.groupby(by=['MONTH'])\n","   monthly_mean=grouped_month.mean(numeric_only=True)\n","   monthly_mean['YEAR']=str(data.loc[0]['YEAR'])  #stores the YEAR\n","  # monthly_mean['MONTH']=str(grouped_month.loc['MONTH'])  #stores the YEAR\n","\n","   daily_head= daily_mean.sort_values(by='MID', ascending=False).head(1)  #dataframe with only hottest average day of the year\n","   monthly_head= monthly_mean      #dataframe with monthly avverages\n","\n","   #update the summary_data dataframe with the next years data\n","   #summary_data=summary_data.append(pd.DataFrame(monthly_head)) #.append for dataframes is being deprecated; use .concat as belwo instead\n","   summary_data = pd.concat([summary_data, pd.DataFrame(monthly_head)], ignore_index=True)\n","\n","\n","\n","\n","# Print info\n","   print(\"%0.2f\",monthly_mean.sort_values(by='MID', ascending=False).head(5))\n","   print(\"\\n\")"],"metadata":{"id":"gmqOt2zJJvry","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_data\n"],"metadata":{"id":"J3A_AA1D_o1G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZkMIX4SdUMD"},"source":["So, what month usually has the highest average temperature?\n","\n","---\n","## Check your understanding\n","\n","You should be able to edit the previous code to output the *hottest* day of each year. When editing code, it's a good idea to use commenting to edit out bits of code rather then deleting lines..."]},{"cell_type":"code","source":[],"metadata":{"id":"NvMB9jV3FiYC"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[{"file_id":"1szcbRiNPm0QSLIBQb7xSj_lts6k5lczi","timestamp":1710934823784}]}},"nbformat":4,"nbformat_minor":0}