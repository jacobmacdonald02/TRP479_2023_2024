{"metadata":{"orig_nbformat":4,"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1jQ-tx_DQ-23Om__ogtN7VY_yFvLXGQk4","timestamp":1710323817296}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Lesson 5: Exploring data using pandas\n","\n","This week,we will learn how to read and explore data files in Python. We will focus on using [pandas](https://pandas.pydata.org/pandas-docs/stable/) which is an open-source package for data analysis in Python. pandas is an excellent toolkit for working with real world data that often have a tabular structure (rows and columns).\n","\n","\n","---\n","\n","\n","## General information\n","\n","### Sources\n","\n","This lesson is inspired by the [Geo-python module at the University of Helsinki](https://geo-python-site.readthedocs.io/en/latest/course-info/course-info.html) which in turn acknowledges the [Programming in Python lessons](http://swcarpentry.github.io/python-novice-inflammation/) from the [Software Carpentry organization](http://software-carpentry.org). This version was adapted for Colab and a UK context by Ruth Hamilton.\n","\n","### About this document\n","\n","This is a [Google Colab Notebook](https://colab.research.google.com/?utm_source=scs-index). This particular notebook is designed to introduce you to a few of the basic concepts of programming in Python. Like other common notebook formats (e.g. [Jupyter](http://jupyterlab.readthedocs.io/en/stable/) ), the contents of this document are divided into cells, which can contain:\n","\n","*   Markdown-formatted text,\n","*   Python code, or\n","*   raw text\n","\n","You can execute a snippet of code in a cell by pressing **Shift-Enter** or by pressing the **Run Cell** button that appears when your cursor is on the cell .\n","\n","---"],"metadata":{"id":"SwOZMsyzbX8t"}},{"cell_type":"markdown","source":["#What is pandas?\n","\n","Pandas is an open-source software library built on top of Python specifically for **data manipulation and analysis**. In particular, pandas offers **data structures** and **operations** for  powerful, flexible, and easy-to-use data analysis and manipulation. Pandas strengthens Python by giving the popular programming language the capability to work with spreadsheet-like data enabling fast loading, aligning, manipulating, and merging, in addition to other key functions."],"metadata":{"id":"sbwruFObQrpt"}},{"cell_type":"markdown","source":["#Pandas data structures\n","\n","We will first get familiar with the **pandas data structures**: *Series* and *DataFrame*:\n","\n","![pandas data structures](https://storage.googleapis.com/lds-media/images/series-and-dataframe.width-1200.png)\n","\n","- **pandas Series** (a 1-dimensional data structure) is used for storing and manipulating a sequence of values. A pandas Series is kind of like a list, but more clever.\n","- **pandas DataFrame** (a 2-dimensional data structure) is used for storing and mainpulating table-like data (data with rows and columns) in Python. You can think of a pandas DataFrame as a programmable spreadsheet. One row or one column in a pandas DataFrame is actually a pandas Series.\n","\n","\n","These pandas structures incorporate a number of things we've already encountered such as:\n","* indices,\n","* data stored in a collection, and\n","* data types.\n","\n","Let's have another look at some pandas data structures below with some additional annotation.\n","\n","![pandas data structures](https://geo-python.github.io/site/_images/pandas-structures-annotated.png)\n","\n","As you can see, both DataFrames and Series in pandas have an **index** that can be used to select values, but they also have **column labels** to identify columns in DataFrames. In the lesson this week we’ll use many of these features to explore real-world data and learn some useful data analysis procedures.\n","\n","For a comprehensive overview of pandas data structures you can have a look at [Chapter 5 in Wes MacKinney’s book Python for Data Analysis](https://wesmckinney.com/book/pandas-basics.html) (3rd Edition, 2022) and the pandas online documentation about [data structures](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html).\n","\n","---\n","\n","\n","> **Note** pandas is a “high-level” package, which means that it makes use of several other packages, such as [NumPy](https://numpy.org/), in the background. There are several ways in which data can be read from a file in Python,  we are focussing primarily on pandas because it is easy-to-use, efficient and intuitive. If you are curious about other approaches for interacting with data files, you can find materials\n"," about reading data using NumPy or built-in Python functions from other sources online.\n","\n","---\n","\n"],"metadata":{"id":"YX6Quckzmj49"}},{"cell_type":"markdown","source":["#First steps with pandas\n","\n","The first step in data analysis is actually getting the data into the platform you are working in! So our first exercise is going to involve using pandas to read in a data file. In this case, we are going to load a csv file that contains some historical weather data from Sheffield."],"metadata":{"id":"ggscmFU0SGy1"}},{"cell_type":"markdown","source":["## Input data: weather statistics\n","\n","Our input data is a text file containing weather observations from Sheffield, UK retrieved from the [CEDA Archive](https://catalogue.ceda.ac.uk/uuid/8bcf6925cddc4681b96f94d424537b9e). This is an open dataset, but you need to be a registered user with CEDA. You should have registered with the site last week and can find a copy of the license [at this link](https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/).\n","\n","- File name: [midas-open_uk-daily-temperature-SY_2020.csv](https://drive.google.com/file/d/1gYjHhdd35K7ZCXQXemDJ34ERapZ33LBD/view?usp=sharing) (have a look at the file before reading it in using pandas!)\n","- The file is available in the L5 folder in the Shared Google drive and will also be posted on Blackboard.\n","- The data file contains observed daily temperature observations for Sheffield for 2020.\n","- There are 732 rows of data in this sample data set.\n","- The data has been derived from a data file of daily temperature measurments downloaded from the [CEDA Archive](https://catalogue.ceda.ac.uk/uuid/8bcf6925cddc4681b96f94d424537b9e).\n","\n","\n","\n"],"metadata":{"id":"EDzwKw7VpCyy"}},{"cell_type":"markdown","source":["## Reading a data file with pandas\n","\n","Now we're ready to read in our temperature data file. **First, we need to import the pandas module.** It is customary to import pandas as `pd`, this means that when we want to use a method from the pandas module, we can shorten the statement to `pd.Method()` rather than `pandas.Method)`."],"metadata":{"id":"P_8f_JvYvJaD"}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"deletable":true,"editable":true,"id":"0lKlqfUSbIzo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, mount your Google Drive so we can access the data in the Shared Google Drive:"],"metadata":{"id":"PGGnWupLvrLc"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"EnOxDewPvp8u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Next, we'll read the input data file**. We will use the `pandas.read_csv()` function to read in the contents of the data file and store it in a *variable* called `data`.\n","\n","\n","> Becasue we are using a Shared Drive, the file path should be the same for everyone. If you want to read a file in your own drive, you can right-click on file name and choose to *Copy Path*\n","\n"],"metadata":{"deletable":true,"editable":true,"id":"WA5anaXvbIzp"}},{"cell_type":"code","source":["# Read the file using pandas\n","data = pd.read_csv(\"/content/drive/Shareddrives/TRP479_Spatial_Data_science_2024/L5/midas-open_uk-daily-temperature-SY_2020.csv\")"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"CC_UoITqbIzq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","> **Delimiter and other optional parameters**\n",">\n","> Our input file is a comma-delimited file; columns in the data are separted by commas (`,`) on each row. The pandas `.read_csv()` function has the comma as the default delimiter so we don't need to specify it separately. In order to make the delimiter visible also in the code for reading the file, could add the `sep` parameter:\n","\n","```\n","data = pd.read_csv('..../midas-open_uk-daily-temperature-SY_2020.csv', sep=`,`)\n","```\n","\n","\n",">The `sep` parameter can be used to specify whether the input data uses some other character, such as `;` as a delimiter. For a full list of available parameters, please refer to the [pandas documentation for pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), or run `help(pd.read_csv)`.\n","\n","---"],"metadata":{"id":"OCzZalubbIzr"}},{"cell_type":"markdown","source":["---\n","\n",">**Reading different file formats**\n",">\n",">`pandas.read_csv()` is a general function for reading data files separated by commas, spaces, or other common separators.\n",">\n",">pandas has several different functions for parsing input data from different formats. There is, for example, a separate function for reading Excel files `read_excel`. Another useful function is `read_pickle` for reading data stored in the [Python pickle format](https://docs.python.org/3/library/pickle.html). Check out the [pandas documentation about input and output functions](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-tools-text-csv-hdf5) and Chapter 6 in [MacKinney (2022): Python for Data Analysis](https://wesmckinney.com/book/accessing-data.html) for more details about reading data.\n","\n","---"],"metadata":{"id":"SEwIXbbMbIzs"}},{"cell_type":"markdown","source":["If all goes as planned, you should now have a new variable `data` in memory that contains the input data.\n","\n","Let's check the the contents of this variable by calling `data` or `print(data)`:"],"metadata":{"id":"PWl11AE9bIzt"}},{"cell_type":"code","source":["data"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"v_hPzoB0bIzu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have some data and it has a structure, but there are some strange values present such as `NaN`, and the first lines of the dataframe look a bit weird. `NaN` stands for \"not a number\", and might indicate some problem with reading in the contents of the file.\n","\n","Plus, we expected about 732 lines of data, but the index values go up to 823 when we print the contents of the `data` variable. Looks like we need to investigate this further."],"metadata":{"deletable":true,"editable":true,"id":"z809IM7ObIzv"}},{"cell_type":"markdown","source":["You should have noted that there are some metadata at the top of the file giving basic information about its contents and source ([midas-open_uk-daily-temperature-SY_2020.csv](https://drive.google.com/file/d/1gYjHhdd35K7ZCXQXemDJ34ERapZ33LBD/view?usp=sharing)). This isn't data we want to process, so we need to skip over that part of the file when we load it.\n","\n","In this case, the first **91** rows contain metadata like this:\n","\n","![image.png](data:image/png;base64,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)\n","\n","\n"],"metadata":{"deletable":true,"editable":true,"id":"BiZEjLh2bIzv"}},{"cell_type":"markdown","source":["Fortunately, skipping over rows is easy to do when reading in data using pandas. We just need to add the `skiprows` parameter when we read the file, listing the number of rows to skip (91 in this case).\n",">We could, of course, delete the metadata from the csv file - but this is not a good idea, the metadata usually contains information we need to interpret the data.\n","\n","Let's try reading the datafile again, and this time defining the `skiprows` parameter."],"metadata":{"deletable":true,"editable":true,"id":"y7kJALl-bIzx"}},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/Shareddrives/TRP479_Spatial_Data_Science_Data/L5/midas-open_uk-daily-temperature-SY_2020.csv\",skiprows=91)"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"9oG1d_NsbIzx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now print the dataframe and see what changed:"],"metadata":{"deletable":true,"editable":true,"id":"5VMS8uRlbIzy"}},{"cell_type":"code","source":["print(data)\n"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"hRcewTaYbIzy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After reading in the data, it is always good to check that everything went well by printing out the data as we did here. However, often it is enough to have a look at the top few rows of the data.\n","\n","We can use the `.head()` function of the pandas DataFrame object to quickly check the top rows. By default, the `.head()` function returns the first 5 rows of the DataFrame:"],"metadata":{"deletable":true,"editable":true,"id":"DBJ09QkMbIzy"}},{"cell_type":"code","source":["data.head()"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"eA4ru28vbIzz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also check the last rows of the data using `data.tail()`:"],"metadata":{"id":"P8LOVyphbIzz"}},{"cell_type":"code","source":["data.tail()"],"metadata":{"id":"D3EroTswbIzz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Note** The last row of our data looks a bit odd. The downloaded dataset uses a 'flag' to indicate where the data starts and finishes. In this case the term 'data' appears in the line before the data starts and 'end data' to indicate the last line. This can be useful if you are reading in files of different sizes and with different amounts of metadata. We don't want to include that last line in our dataset; but before we work out how to remove it, we'll do a bit more exploring.\n","\n","\n","\n"],"metadata":{"id":"02nG6FxdAe8h"}},{"cell_type":"markdown","source":[">**Note** that pandas DataFrames have **labelled axes (rows and columns)**. In our sample data, the rows labeled with an index value (`0` to `732`), and columns labelled `on_end_time`, `id_type`, `id`, etc. Later on, we will learn how to use these labels for selecting and updating subsets of the data.\n"],"metadata":{"deletable":true,"editable":true,"id":"utYM-KQTbIzz"}},{"cell_type":"markdown","source":["**Let's also confirm the data type of our data variable:**"],"metadata":{"deletable":true,"editable":true,"id":"W2pPC-R8bIzz"}},{"cell_type":"code","source":["type(data)"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"xoRz75tmbIz0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["No surprises here, our data variable is a pandas DataFrame."],"metadata":{"deletable":true,"editable":true,"id":"EElBPCXRbIz0"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","#### Reading subsets of a file\n","\n","This time, we are going to read in the file `midas-open_uk-daily-temperature-SY_2020.csv` again and store its contents in a new variable called `temp_data`. In this case, we only want to read in the columns `ob_end_time` and `id_type`. You can achieve this using an additional paramter, `usecols`, when reading in the file. For example, the parameter `..., skiprows=91,usecols[0,1])` would read *only* the first two columns. Feel free to check for more help in the [pandas.read_csv documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)."],"metadata":{"id":"08c92mOKbIz0"}},{"cell_type":"code","source":["# Complete the line below to read in the data file again BUT  only the first two columns ('ob_end_time' and 'id_type')\n","temp_data = pd.read_csv("],"metadata":{"id":"lQ82qULmbIz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Click here to see code\n","temp_data = pd.read_csv(\"/content/drive/Shareddrives/TRP479_Spatial_Data_Science_Data/L5/midas-open_uk-daily-temperature-SY_2020.csv\",skiprows=91,usecols=[0,1])"],"metadata":{"cellView":"form","id":"k9Rmdo9cHco-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now check that is read in only the first two columns:"],"metadata":{"id":"KWCMTVcYH0Le"}},{"cell_type":"code","source":["# Check the contents of the first rows of the data file\n","temp_data.head()"],"metadata":{"id":"VAX6IeHeHp_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Click here to see code\n","# Check the contents of the first rows of the data file\n","temp_data.head()"],"metadata":{"id":"V1N2I0WXbIz1","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## DataFrame properties\n","\n","Let's continue with the full data set that we have stored in the variable `data` and explore it's contents further.\n","A normal first step when you load new data is to explore the dataset a bit to understand how the data is structured, and what kind of values are stored in there."],"metadata":{"deletable":true,"editable":true,"id":"lXG_9meWbIz1"}},{"cell_type":"markdown","source":["Let's start by checking the size of our data frame. We can use the `len()` function similar to the one we use with lists to check how many rows we have:"],"metadata":{"id":"GrZuFYbGbIz1"}},{"cell_type":"code","source":["# Check the number of rows\n","len(data)"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"pQFxNyDubIz1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also get a quick sense of the size of the dataset using the `shape` attribute.\n"],"metadata":{"deletable":true,"editable":true,"id":"jmy20ZGlbIz2"}},{"cell_type":"code","source":["# Check dataframe shape (number of rows, number of columns)\n","data.shape"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"Jj8wY7UbbIz2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we see that our dataset has 732 rows and 22 columns, just as we saw above when printing out the entire DataFrame."],"metadata":{"deletable":true,"editable":true,"id":"1b5rOR60bIz2"}},{"cell_type":"markdown","source":["\n",">**Note** `shape` is one of the several [attributes related to a pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#attributes-and-underlying-data)."],"metadata":{"id":"_tHLjVPXbIz3"}},{"cell_type":"markdown","source":["**We can also check the column names we have in our DataFrame.** We already saw the column names when we checked the 5 first rows using `data.head()`, but often it is useful to access the column names directly. You can check the column names by calling `data.columns` (returns an index object that contains the column labels) or `data.columns.values` (returns an numpy array object that contains the column labels) :"],"metadata":{"id":"cvwu-VoDbIz3"}},{"cell_type":"code","source":["# Print column names\n","data.columns\n","\n","#data.columns.values\n","data.columns.values\n"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"Y0AUWusYbIz3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also find information about the row identifiers using the `index` attribute:"],"metadata":{"deletable":true,"editable":true,"id":"jmFRc9u0bIz4"}},{"cell_type":"code","source":["# Print index\n","data.index\n"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"0fvQbn0sbIz4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we see how the data is indexed, starting at 0, ending at 731, and with an increment of 1 between each value. This is basically the same way in which Python lists are indexed, however, pandas also allows other ways of identifying the rows. DataFrame indices could, for example, be character strings, or date objects. We will learn more about resetting the index later."],"metadata":{"deletable":true,"editable":true,"id":"tl7JQ8LKbIz4"}},{"cell_type":"markdown","source":["What about the data types of each column in our DataFrame? We can check the data type of all columns at once using `pandas.DataFrame.dtypes`:"],"metadata":{"deletable":true,"editable":true,"id":"TeILq1T7bIz4"}},{"cell_type":"code","source":["# Print data types\n","data.dtypes"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"O6KFxwLrbIz4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we see that the data types in our columns are a mix of `object` and `float64` (decimal values with 64-bit precision)."],"metadata":{"deletable":true,"editable":true,"id":"Dw3Qvb3vbIz4"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","#### Check your understanding\n","\n","See if you can find a way to print out the number of columns in our DataFrame."],"metadata":{"id":"mdX1Cq2EbIz5"}},{"cell_type":"code","source":["#find the number of columns in the 'data' dataframe:\n"],"metadata":{"id":"hg80CZGPJ9v5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Click here to see one solution\n","# Here is one solution\n","len(data.columns)"],"metadata":{"tags":["hide-cell"],"id":"orollzNCbIz5","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","### Selecting columns\n","\n","We can select specific columns based on the column lables. The basic syntax is `dataframe[label]`, where `label` can be a single column name, or a list of column names. Let's start by selecting two columns, `'ob_end_time'` and `'max_air_temp'`. Becasue we want **2** columns, we need to place them in a list first, suing the syntax `[\"ob_end_time\", \"max_air_temp\"]`."],"metadata":{"id":"67sO3pwfbIz5"}},{"cell_type":"code","source":["selection = data[[\"ob_end_time\", \"max_air_temp\"]]"],"metadata":{"id":"sOdir_CBKTrA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selection"],"metadata":{"id":"6lB6TkOWbIz5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's also check the data type of this selection:"],"metadata":{"id":"BP4DL-hwbIz5"}},{"cell_type":"code","source":["type(selection)"],"metadata":{"id":"UGgKkpZlbIz5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The subset is still a pandas DataFrame, and we are able to use all the methods and attributes related to a pandas DataFrame also with this subset. For example, we can check the shape:"],"metadata":{"id":"Y3V3u_TqbIz6"}},{"cell_type":"code","source":["selection.shape"],"metadata":{"id":"dS91OWZobIz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also access a single column of the data based on the column name:"],"metadata":{"deletable":true,"editable":true,"id":"RiTd7EWMbIz6"}},{"cell_type":"code","source":["selection[\"max_air_temp\"]"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"ZYySBcVgbIz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What about the type of the column itself?"],"metadata":{"id":"-YeRorPbbIz6"}},{"cell_type":"code","source":["# Check datatype of the column\n","type(selection[\"max_air_temp\"])"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"SWdrmrNPbIz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n",">**Each column (and each row) in a pandas data frame is actually a pandas Series** - a 1 dimensional data structure!\n","\n","---"],"metadata":{"deletable":true,"editable":true,"id":"pV-TPfBnbIz7"}},{"cell_type":"markdown","source":["---\n","**Note**\n",">You can also retreive a column using a different syntax:\n"," >   \n",">```\n","data.TEMP\n","```\n",">\n",">This syntax works only if the column name is a valid name for a Python variable (e.g. the column name should not contain whitespace).\n","The syntax `data[\"column\"]` works for all kinds of column names, so we recommend using this approach.\n","\n","---"],"metadata":{"id":"KS-HW0jLbIz7"}},{"cell_type":"markdown","source":["### Dropping data from a dataframe\n","For now, let's go back to that last row of data; we would  like to remove the last row becasue it doesn't contain valid data.\n","\n","We can do that using the `.drop()` method on the DataFrame. [This method](https://www.w3schools.com/python/pandas/ref_df_drop.asp) can remove a specified row or column (or multiple rows or columns).\n","\n","By specifying the `axis` parameter, we can specify if we want to 'drop' a column  (`axis='columns'`) or a row (`axis='index'`). The parameter `inplace = True` removes the column or row from the current dataframe and is the default, aetting the `inplace` parameter to `False` returns a *copy* of the dataframe with the column or row removed."],"metadata":{"id":"YBHnUVabpYJa"}},{"cell_type":"code","source":["#look at the last rows of the data frame\n","data.tail()"],"metadata":{"id":"9_eARKldrF0n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 'drop' the last row of data in our data frame\n","data.drop(len(data)-1,axis = 'index',inplace =True)"],"metadata":{"id":"a-9mrj5appCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Note** Here, rather than specify the row we want to drop by its index number, we use the `len()` function to calculate the *length* of the data (and subtract 1 to account for zero-based indexing)."],"metadata":{"id":"iMc8kbLmqcOW"}},{"cell_type":"code","source":["#check the last rows of the data frame; it should end with two values for the 31/12/2020\n","data.tail()"],"metadata":{"id":"uuZx6nkHsDUi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## Descriptive statistics\n","\n","pandas DataFrames and Series contain useful methods for getting summary statistics. Available methods include `mean()`, `median()`, `min()`, `max()`, and `std()` (the standard deviation).\n","\n","We could, for example, check the mean temperature in our input data. We check the mean for a single column (*Series*):"],"metadata":{"id":"le7ZfxUZbIz7"}},{"cell_type":"code","source":["# Check mean value of a column\n","data[\"max_air_temp\"].mean()"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"mjHsWSXsbIz7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["and for all columns (in the *DataFrame*):"],"metadata":{"id":"qneIG3JYbIz7"}},{"cell_type":"code","source":["# Check mean value for all columns\n","data.mean()"],"metadata":{"id":"AEA92Mn0bIz8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For an overview of the basic statistics for all attributes in the data, we can use the `describe()` method:\n"],"metadata":{"deletable":true,"editable":true,"id":"9jIlza4dbIz8"}},{"cell_type":"code","source":["# Get descriptive statistics\n","data.describe()"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"-o6N1zgPbIz8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Check your understanding\n","\n","It doesn't make much sense to print out descriptive statistics for some of our  columns (`'src_id'` is the identifying code for the Sheffiled weather station and is the same for all entires). You'll also note some of the columns (`'ob_end_time'`, for example) are not listed. This is beacsue they are **datetime** objects, which we will learn about later.\n","\n","See if you can print out the descriptive statistics again, this time only for columns `'max_air_temp'` and `'min_air_temp'` (**Tip:** go back to the section on Selecting Columns if you are not sure):"],"metadata":{"id":"nvC-SIlFbIz8"}},{"cell_type":"code","source":["# Get descriptive statistics for selected columns\n"],"metadata":{"id":"F1F7YmKebIz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Click to show code:\n","data[[\"max_air_temp\", \"min_air_temp\"]].describe()"],"metadata":{"cellView":"form","id":"ugAD9Gc3lomi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## Extra: Very basic plots\n","\n","Visualizing the data is a key part of data exploration, and pandas comes with a handful of plotting methods, which all rely on the [Matplotlib](https://matplotlib.org/) plotting library.\n","\n","For very basic plots, we don’t need to import Matplotlib separately. We can already create very simple plots using the `DataFrame.plot` method.\n","\n","> **NOTE** You may also have noticed the 'Suggest Charts' button next to dataFrame outputs. These are a useful short cut to view charts associated with the dataFrame data and also provide the underlying code snippet. **However, it is important to also understand how the plots are created and the underlying code so that you can control the contents and structure of the plots.**\n","You may also find that the 'suggested' plots are not always meaningful - thinking critically about what you choose to plot is important!\n","\n","Let's plot all the columns that contain values related to temperatures:"],"metadata":{"id":"VGHA8zr0bIz9"}},{"cell_type":"code","source":["data[[\"max_air_temp\", \"min_air_temp\"]].plot()"],"metadata":{"id":"mrsmRBimbIz9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now you might want to start modifying the plot by adding axis labels, color settings and other formatting. We will learn all this later!"],"metadata":{"id":"a5D1Bs2NbIz-"}},{"cell_type":"markdown","source":["## Extra: From lists to pandas objects\n","\n","Most often we create pandas objects by reading in data from an external source, such as a text file. Here, we will briefly see how you can create pandas objects from Python lists. If you have long lists of numbers, for instance, creating a pandas Series will allow you to interact with these values more efficiently in terms of computing time."],"metadata":{"deletable":true,"editable":true,"id":"BmzcgdLIbIz-"}},{"cell_type":"code","source":["# Create pandas Series from a list\n","number_series = pd.Series([ 4, 5, 6, 7.0])\n","print(number_series)"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"Vt-yhD4ebIz-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that pandas is smart about the conversion, detecting a single floating point value (`7.0`) and assigning all values in the Series the data type `float64`."],"metadata":{"deletable":true,"editable":true,"id":"o5cshFg0bIz-"}},{"cell_type":"markdown","source":["If needed, you can also set a custom index when creating the object:"],"metadata":{"id":"ZIgV-q6EbIz_"}},{"cell_type":"code","source":["number_series = pd.Series([ 4, 5, 6, 7.0], index=['a','b','c','d'])\n","print(number_series)"],"metadata":{"id":"bSuCH33EbIz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(number_series)"],"metadata":{"id":"fEs0q5GfbIz_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How about combining several lists as a DataFrame? Let's take a subset of the lists we used in Exercise 2 (L2) and see how we could combine those as a pandas DataFrame:"],"metadata":{"id":"s6o_eBj2bIz_"}},{"cell_type":"code","source":["# Station names\n","stations = ['Braemar','Dunstaffnage','Eskdalemuir','Leuchars','Nairn','Newton Rigg','Paisley','Wick Airport']\n","\n","# Latitude coordinates of Weather stations\n","lats = [57.01,56.45,55.31,56.38,57.59,54.67,55.85,58.45]\n","\n","# Longitude coordinates of Weather stations\n","lons = [-3.40,-5.44,-3.21,-2.86,-3.82,-2.79,-4.43,-3.09]"],"metadata":{"id":"Aqt7mauCbI0A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Often we indeed create pandas DataFrames by reading in data (e.g. using `pd.read_csv(filename.csv)`), but sometimes you might also combine lists into a DataFrame inside the script using the `pandas.DataFrame` constructor. Here, we are using a *Python dictionary* `{\"column_1\": list_1, \"column_2\": list_2, ...}` to indicate the structure of our data."],"metadata":{"id":"mszpl9IQbI0A"}},{"cell_type":"code","source":["new_data = pd.DataFrame(data = {\"station_name\" : stations, \"lat\" : lats, \"lon\" : lons})\n","new_data"],"metadata":{"id":"vXMW55AhbI0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(new_data)"],"metadata":{"id":"ommuFuu0bI0B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the previous example, we created a data frame from existing lists. Often, however, you might want to start working with an **empty** data frame instead:"],"metadata":{"id":"zLPQcRL8bI0B"}},{"cell_type":"code","source":["df = pd.DataFrame()"],"metadata":{"id":"UoeYkJH3bI0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df)"],"metadata":{"id":"zrVzfGD4bI0B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check more details about available paramenters and methods from [the pandas.DataFrame documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas-dataframe)."],"metadata":{"id":"B8aIJgkLbI0C"}},{"cell_type":"markdown","source":["## Extra: Interactive tables:\n","You may have noticed a little 'table' icon when you output a pandas dataframe, this opens your dataframe as an [interactive table](\n","https://colab.research.google.com/notebooks/data_table.ipynb). This allows you  filter, sort, and explore  your data dynamically.\n","\n","Experiment with this feature with the `new_data` dataset and answer the questions below."],"metadata":{"id":"3VuAtn2Cwn7Y"}},{"cell_type":"code","source":["#convert 'new_data' to an interactive table\n","new_data\n"],"metadata":{"id":"vg7j1k-OyCwE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","*   Which is the most *northerly* weather station?\n","*   Which is the most *westerly* weather station?\n","* Filter the `new_data` to show only the weather stations with a latitude between 55 and 57\n","\n"],"metadata":{"id":"DURQPkOzyd4q"}}]}