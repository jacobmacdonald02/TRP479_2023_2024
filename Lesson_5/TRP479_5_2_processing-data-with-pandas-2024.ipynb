{"metadata":{"orig_nbformat":4,"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1jQJ3fT_h8FqS3V0fIRj3Atwlf3KweoTc","timestamp":1710332577397}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Lesson 6: Processing data with pandas\n","\n","---\n","\n","\n","## General information\n","\n","### Sources\n","\n","This lesson is inspired by the [Geo-python module at the University of Helsinki](https://geo-python-site.readthedocs.io/en/latest/course-info/course-info.html) which in turn acknowledges the [Programming in Python lessons](http://swcarpentry.github.io/python-novice-inflammation/) from the [Software Carpentry organization](http://software-carpentry.org). This version was adapted for Colab and a UK context by Ruth Hamilton.\n","\n","### About this document\n","\n","This is a [Google Colab Notebook](https://colab.research.google.com/?utm_source=scs-index). This particular notebook is designed to introduce you to a few of the basic concepts of programming in Python. Like other common notebook formats (e.g. [Jupyter](http://jupyterlab.readthedocs.io/en/stable/) ), the contents of this document are divided into cells, which can contain:\n","\n","*   Markdown-formatted text,\n","*   Python code, or\n","*   raw text\n","\n","You can execute a snippet of code in a cell by pressing **Shift-Enter** or by pressing the **Run Cell** button that appears when your cursor is on the cell .\n","\n","---\n","\n","During the first part of this lesson you learned the basics of pandas data structures (*Series* and *DataFrame*) and got familiar with basic methods loading and exploring data.\n","Here, we will continue with basic data manipulation and analysis methods such calculations and selections.\n","\n","We are now working in a new notebook file and we need to import pandas again."],"metadata":{"deletable":true,"editable":true,"id":"t2iX5flO0HX-"}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"GQWoO7Di0HYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"sNStErlTRemn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's work with the same input data `'midas-open_uk-daily-temperature-SY_2020.csv'` and load it using the `pd.read_csv()` method. Remember, that the first 91 lines contain metadata so we can skip those. This time, let's store the filepath into a separate variable in order to make the code more readable and easier to change afterwards:"],"metadata":{"id":"FBkayI6t0HYF"}},{"cell_type":"code","source":["# Define file path:\n","fp = r'/content/drive/Shareddrives/TRP479_Spatial_Data_Science_Data/L5/midas-open_uk-daily-temperature-SY_2020.csv'\n","\n","# Read in the data from the file (starting at row 92):\n","data = pd.read_csv(fp, skiprows=91,infer_datetime_format=True)"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"uC3blEob0HYF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Filepaths**\n",">Note, that our input file `'midas-open_uk-daily-temperature-SY_2020.csv'` is located **in a different folder** to the notebook we are running. So we are using an **absolute filepath** to access the input data file, `/content/drive/Shareddrives/TRP479_Spatial_Data_Science_Data/L5/midas-open_uk-daily-temperature-SY_2020.csv`.\n","> When working with absolute filepaths, it's good practice to pass the file paths as a [raw string](https://docs.python.org/3/reference/lexical_analysis.html#literals) using the prefix `r` in order to avoid problems with escape characters such as `\"\\n\"`.\n","\n","\n",">If the file was in the same folder as the working directory for our Python session, we could use a **relative filepath** and would only need to pass the filename\n"," to `.read_csv()` function (you can see the working directory using the `%pwd`magic command)\n"],"metadata":{"id":"n-0sfAXC0HYG"}},{"cell_type":"markdown","source":["Remember to always check the data after reading it in:"],"metadata":{"id":"Dv2W2hSE0HYG"}},{"cell_type":"code","source":["data.head()\n"],"metadata":{"id":"RmYW3NzR0HYG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.tail()"],"metadata":{"id":"Obn52EU91ZCi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remember, we also need to remove the last row..."],"metadata":{"id":"v_920QsU6raM"}},{"cell_type":"code","source":["data.drop(len(data)-1,axis='index',inplace=True)"],"metadata":{"id":"Tt_2yiat6yIC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">**Note** If you repeat the .drop() command, it will keep dropping the last row and your data will get smaller and smaller...\n","\n"],"metadata":{"id":"WMtHsGHp8uVk"}},{"cell_type":"markdown","source":["## Selecting rows and columns\n","\n","We often want to select only specific rows from a DataFrame for further analysis. There are multiple ways of selecting subsets of a pandas DataFrame. In this section we will go through the most useful tricks for selecting specific rows, columns and individual values.\n","\n"],"metadata":{"deletable":true,"editable":true,"id":"5bpYVuHZ0HYM"}},{"cell_type":"markdown","source":["###Selecting several columns\n","\n","We looked at this in the previous notebook; the basic syntax is `dataframe[label]`, where label can be a single column name, or a *list* of column names. Let's start by selecting the following columns, `'ob_end_time', 'max_air_temp', 'min_air_temp'` and `'ob_hour_count'`; remember, to specify a list, we need to put our column names into a list using `[...]` *before* using the dataframe syntax."],"metadata":{"id":"RZM_hU8T_yzr"}},{"cell_type":"code","source":["selection_c=data[[\"ob_end_time\",\"max_air_temp\",\"min_air_temp\",\"ob_hour_count\",\"date\",\"time\"]]"],"metadata":{"id":"3-lh6iaJAgzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selection_c"],"metadata":{"id":"3oSFboLxOueV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Selecting several rows\n","\n","One common way of selecting only specific rows from your DataFrame is done via **index slicing** to extract part of the DataFrame. Slicing in pandas can be done in a similar manner as with normal Python lists, i.e. you specify the index range you want to select inside the square brackets: ``dataframe[start_index:stop_index]``.\n","\n","Let's select the first five rows and assign them to a variable called `selection_r`:"],"metadata":{"id":"27RQBqsG_v2u"}},{"cell_type":"code","source":["# Select first five rows of dataframe using row index values\n","selection_r=data[0:5]\n","selection_r"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"rS3XtVjt0HYM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">**Note** Here we have selected the first **five** rows (index 0-4) using the integer index. Slicing is *inclusive* of the start and *exclusive* of the stop.\n"],"metadata":{"deletable":true,"editable":true,"id":"gzeZDfwy0HYM"}},{"cell_type":"markdown","source":["### Selecting several rows and columns\n","\n","It is also possible to control which columns are chosen when selecting a subset of rows. In this case we will use  [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) which selects data based on **axis labels** (row labels and column labels).\n","\n","Let's select temperature values (column `max_air_temp`) from rows 0-5:"],"metadata":{"deletable":true,"editable":true,"id":"LgP1zeAv0HYM"}},{"cell_type":"code","source":["# Select temp column values on rows 0-5\n","selection = data.loc[0:5, \"max_air_temp\"]\n","selection"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"PJk7aQEL0HYN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">**Note**\n","In this case, we get **six** rows of data (index 0-5)! **We are now doing the selection based on axis labels instead of the integer index.**\n"],"metadata":{"id":"7qBIM63f0HYN"}},{"cell_type":"markdown","source":["It is also possible to select multiple columns when using `loc`. Here, we select the `TEMP` and `TEMP_CELSIUS` columns from a set of rows by passing them inside a list (`.loc[start_index:stop_index, list_of_columns]`):"],"metadata":{"deletable":true,"editable":true,"id":"FcLuqdHW0HYN"}},{"cell_type":"code","source":["# Select columns \"max_air_temp\" and \"min_air_temp\" on rows 0-5\n","selection = data.loc[0:5, [\"max_air_temp\", \"min_air_temp\"]]\n","selection"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"1BlqIJal0HYN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Selecting a single row\n","\n","You can also select an individual row from a specific position using the `.loc[]` indexing. Here we select all the data values using index 4 (the 5th row):"],"metadata":{"id":"qCeWS6BE0HYO"}},{"cell_type":"code","source":["# Select one row using index\n","row = data.loc[4]\n","row"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"lvmI-lGV0HYO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["``.loc[]`` indexing returns the values from that position as a ``pd.Series`` where the indices are actually the column names of those variables. Hence, you can access the value of an individual column by referring to its index using the following format (both should work):\n"],"metadata":{"deletable":true,"editable":true,"id":"5BHaaeSA0HYO"}},{"cell_type":"code","source":["# Print one attribute from the selected row\n","row[0]\n","row[\"ob_end_time\"]\n","\n","print(row[0])\n","print(row[\"ob_end_time\"])"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"QMXQdxir0HYO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Selecting a single value based on row and column\n","\n","Sometimes it is enough to access a single value in a DataFrame. In this case, we can use [DataFrame.at](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html#pandas-dataframe-at) instead of `Data.Frame.loc`.\n","\n","Let's select the temperature (column `max_air_temp`) on the 7th row (index `6`) of our DataFrame."],"metadata":{"id":"Y8dRGYTY0HYP"}},{"cell_type":"code","source":["row_col=data.at[6,\"max_air_temp\"]\n","row_col"],"metadata":{"id":"Du2Q2oERTuV7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's check that worked; the data in row *6* is:"],"metadata":{"id":"YflJDryCULWO"}},{"cell_type":"code","source":["#the data in row 6:\n","row=data.loc[6]"],"metadata":{"id":"0EhlwT810HYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#the value at index \"max_air_temp\" in row 6 is\n","row[\"max_air_temp\"]"],"metadata":{"id":"mZs_P1CEEIrm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EXTRA: Selections by integer position\n","\n",">**` .iloc`**\n",">\n",">`.loc` and `.at` are based on the **axis labels** - the names of columns and rows. Axis labels can be also something other than \"traditional\" index values. For example, datetime is commonly used as the row index.\n","`.iloc` is another indexing operator which is based on *integer value* indices. Using `.iloc`, it is possible to refer also to the columns based on their index value. For example,  `data.iloc[0,0]` would return `01/01/2020 09:00` in our example data frame.\n"," >   \n",">See the pandas documentation for more information about [indexing and selecting data](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-and-selecting-data).\n","\n","\n","For example, we could select `max_air_temp` and the `min_air_temp` columns from a set of rows based on their index."],"metadata":{"id":"IxWyEEl30HYP"}},{"cell_type":"code","source":["data.iloc[0:5:,6:8]"],"metadata":{"id":"Hop0zFln0HYP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To access the value on the first row and 8th column (`max_air_temp`), the syntax for `iloc` would be:\n","    "],"metadata":{"id":"KPCcMU4V0HYP"}},{"cell_type":"code","source":["data.iloc[0,6]"],"metadata":{"id":"kx8Iqb7-0HYP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also access individual rows using `iloc`. Let's check out the last row of data:"],"metadata":{"id":"vdeVxjPq0HYQ"}},{"cell_type":"code","source":["data.iloc[-1]"],"metadata":{"id":"zaJz5rFx0HYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QJytee_MVxja"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The difference approaches used here (`.loc, .iloc, .at`) can often be used to get the same results, however, there are important differences. One of the neatest explanations of what the (subtle) diffferences are comes from [this answer to a Stackexchange question.](https://stackoverflow.com/questions/28757389/pandas-loc-vs-iloc-vs-at-vs-iat):\n","\n","Essentially, `.at` and `.iat` are meant to access a scalar, that is, a *single* element in the dataframe, while `.loc` and `.iloc` are meant to access *several* elements at the same time:\n","* `.loc`: only works on *index*\n","* `.iloc`: works on *position*\n","* `.at`: gets a *scalar* (single) value (it's a very fast `.loc`)\n","* `.iat`: gets a *scalar* (single) value (it's a very fast `.iloc`)"],"metadata":{"id":"jrbKIDBWVUkR"}},{"cell_type":"markdown","source":["## Basic calculations\n","\n","One of the most common things to do in pandas is to create new columns based on calculations between different variables (columns).\n","\n","We can create a new column `DIFF` in our DataFrame by specifying the name of the column and giving it some default value (in this case the decimal number `0.0`). To make things easier, we are going to use the `selection_c` dataframe we made earlier which only contains the 'ob_end_time', 'max_air_temp', 'min_air_temp' , 'ob_hour_count' , 'date' and 'time' columns."],"metadata":{"deletable":true,"editable":true,"id":"FEmuix_q0HYH"}},{"cell_type":"code","source":["selection_c"],"metadata":{"id":"FVYW892OXokY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a new column \"DIFF\"\n","selection_c[\"DIFF\"] = 0.0\n","\n","# Check how the dataframe looks like:\n","selection_c"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"V5whFOEI0HYI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's check the datatype of our new column:"],"metadata":{"deletable":true,"editable":true,"id":"85diPF7J0HYI"}},{"cell_type":"code","source":["selection_c[\"DIFF\"].dtypes"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"GogofKUo0HYJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okey, so we see that pandas created a new column and recognized automatically that the data type is float as we passed a 0.0 value to it.\n","\n","Let's update the column `DIFF` by calculating the difference between `MAX` and `MIN` columns to get an idea how much the temperatures have been varying during different days:"],"metadata":{"deletable":true,"editable":true,"id":"iXKUE2k_0HYJ"}},{"cell_type":"code","source":["# Calculate max min difference\n","selection_c[\"DIFF\"] = selection_c[\"max_air_temp\"] - selection_c[\"min_air_temp\"]\n","\n","# Check the result\n","selection_c.head()"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"c2pMdA5C0HYK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The calculations were stored into the `DIFF` column as planned.\n","\n","You can also create new columns on-the-fly at the same time when doing the calculation (the column does not have to exist before). Furthermore, it is possible to use any kind of math\n","algebra (e.g. subtraction, addition, multiplication, division, exponentiation, etc.) when creating new columns.\n","\n","We can for example convert the Celsius temperatures in the `max_air_temp` column into Fahreneit using the formula that we have seen before.\n","\n",">`°F` equals `°C` multiplied by `9/5`, plus `32`.\n","\n","Let's do that and store it in a new column called `TEMP_F`."],"metadata":{"deletable":true,"editable":true,"id":"Nw_p4w920HYK"}},{"cell_type":"code","source":["# Create a new column and convert temp fahrenheit to celsius:\n","selection_c[\"TEMP_F\"] = (selection_c[\"max_air_temp\"] * 9/5) +32\n","\n","# Check output\n","selection_c.head()"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"ILnK-j_y0HYL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Filtering and updating data\n","\n","One really useful feature in pandas is the ability to easily filter and select rows based on a conditional statement.\n","The following example shows how to select rows when the Celsius temperature has been higher than 15 degrees into variable `warm_temps` (warm temperatures). pandas checks if the condition is `True` or `False` for each row, and returns those rows where the condition is `True`:"],"metadata":{"deletable":true,"editable":true,"id":"5Rf-rDOY0HYQ"}},{"cell_type":"code","source":["# Check the condition\n","selection_c[\"max_air_temp\"] > 15"],"metadata":{"id":"VZmDXDFW0HYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select rows with temp celsius higher than 15 degrees\n","warm_temps = selection_c.loc[selection_c[\"max_air_temp\"] > 15]\n","warm_temps"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"Fk2Jp99p0HYQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It is also possible to combine multiple criteria at the same time. You shoud have noticed by now that we have **two** recordings for each day:\n","* one records the maximum and minimum temperatures recorded in the 12 hours before 09:00 and\n","* one for the maximum and minimum temperatures recorded in the 12 hours before 21:00."],"metadata":{"id":"Y7SAvd0kaWv0"}},{"cell_type":"code","source":["#check the values in the \"ob_end_time\" column...\n","selection_c.head()"],"metadata":{"id":"iuyluAG4aahM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","This information is stored in `'ob_end_time'`. But the dataset also has a variable with the `date` and one with the `time`. We can use these to select the *daytime*  temperature readings;** i.e. those recorded in the 12 hours before 21:00.**\n","\n","Here, we select temperatures above 15 degrees that were recorded on the during *daytime* (i.e. `time = 21:00`).\n","\n","Combining multiple criteria can be done with the `&` operator (AND) or the `|` operator (OR). Notice, that it is often useful to separate the different clauses inside  parentheses `()`."],"metadata":{"deletable":true,"editable":true,"id":"n-KgQesl0HYR"}},{"cell_type":"code","source":["# Select rows with temp celsius higher than 15 degrees in the 12h before 21:00\n","warm_temps = selection_c.loc[(data[\"max_air_temp\"] > 15) & (selection_c[\"time\"] == \"21:00:00\")]\n","warm_temps"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"QeceIbey0HYR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we have a subset of our DataFrame with only rows where the `max_air_temp` is above 15 and the time in `time` column is 21:00:00.\n","\n","Notice, that the index values (numbers on the left) are still showing the positions from the original DataFrame. It is possible to **reset** the index using `reset_index()` function that\n","might be useful in some cases to be able to slice the data in a similar manner as above. By default the `reset_index()` would make a new column called `index` to keep track of the previous\n","index which might be useful in some cases but not here, so we can omit that by passing parameter `drop=True`."],"metadata":{"deletable":true,"editable":true,"id":"eyYYP-HO0HYR"}},{"cell_type":"code","source":["# Reset index\n","warm_temps = warm_temps.reset_index(drop=True)\n","warm_temps"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"mi_zMiSk0HYR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As can be seen, now the index values goes from 0 to 146."],"metadata":{"id":"aNHpmSoe0HYR"}},{"cell_type":"markdown","source":["#### Check your understanding\n","\n","Find the number of days that minimum *overnight* temperature (in Celsius) was below 0. This time you should select the rows based on a condition for the `time` column!"],"metadata":{"id":"DKZQp-Yi0HYR"}},{"cell_type":"code","source":["# Find the number of days that minimum overnight temperature (in Celsius) was below 0"],"metadata":{"id":"3Ii7NcsD0HYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Click to show code\n","cold_temps = selection_c.loc[(selection_c[\"min_air_temp\"] < 0) & (selection_c[\"time\"] == \"09:00:00\")]\n","cold_temps"],"metadata":{"cellView":"form","id":"S5PWRgy1pQ4E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","**Deep copy**\n",">In this lesson, we have stored subsets of a DataFrame as a new variable. In some cases, we are still referring to the original data and any modifications made to the new variable might influence the original DataFrame.\n"," >   \n",">If you want to be extra careful to not modify the original DataFrame, then you should take a proper copy of the data before proceeding using the `.copy()` method. You can read more about indexing, selecting data and deep and shallow copies in [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) and in [this excellent blog post](https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-part-4-c4216f84d388).\n","\n","---"],"metadata":{"id":"sTbEy8Dj0HYS"}},{"cell_type":"markdown","source":["## Dealing with missing data\n","\n","You may have noticed by now, we have several missing values for the `max_air_temp`, `min_air_temp` and difference columns. These missing values are indicated as `NaN` (not a number). Having missing data in your datafile is really common situation and typically you want to deal with it somehow. Common procedures to deal with `NaN` values are to either **remove** them from the DataFrame or **fill** them with some value. In pandas both of these options are really easy to do.\n","\n","### the dropna() function\n","Let's first see how we can remove the NoData values (i.e. clean the data) using the [.dropna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) function. Inside the function you can pass a list of column(s) from which the `NaN` values should found using the `subset` parameter."],"metadata":{"deletable":true,"editable":true,"id":"6D-wOBYw0HYS"}},{"cell_type":"code","source":["#first let's make a copy of the selection_c dataframe\n","selection_c_copy= selection_c.copy()"],"metadata":{"id":"dCFrmrBJgFnp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selection_c.loc[0:10]"],"metadata":{"id":"4nlirPF4s3ts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selection_c_clean = selection_c.dropna(subset=[\"max_air_temp\"])\n","selection_c_clean.loc[0:10]"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"WmmcEjO30HYS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can see by looking at the table above (and the change in index values), we now have a DataFrame without the NoData values in the `max_air_temp` column.\n","\n","\n",">**Note** that we replaced the original `selection_c` variable with version where no data are removed. The `.dropna()` function, among other pandas functions can also be applied \"inplace\" which means that the function updates the DataFrame object and returns `None`:\n","    \n",">\n",">`selection_c.dropna(subset=['max_air_temp'], inplace=True)`\n",">\n","> We can also subset with more than one column:\n",">\n",">`selection_c.dropna(subset=['max_air_temp','min_air_temp'], inplace=True)`\n","\n"],"metadata":{"deletable":true,"editable":true,"id":"5bhP6slf0HYS"}},{"cell_type":"code","source":["selection_c.dropna(subset=['max_air_temp','min_air_temp'], inplace=True)\n","selection_c.loc[0:10]"],"metadata":{"id":"G6G99CEmfQjO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that now we have changed the contents of the original `selection_c` dataframe..."],"metadata":{"id":"E1D5b_l7fjfb"}},{"cell_type":"markdown","source":["### the fillna() function\n","Another option is to fill the NoData with some value using the `fillna()` function. Here we can fill the missing values in the with value -9999. Note that we are not giving the `subset` parameter this time but we are specifying the `'inplace'` parameter.\n","\n","We'll use the copy of `selection_c` that we made earlier."],"metadata":{"id":"fqSLma_WfOaR"}},{"cell_type":"code","source":["#check the contents of selection_c_copy\n","selection_c_copy[0:10]"],"metadata":{"id":"zyeJEcYygpZ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fill na values\n","selection_c_copy.fillna(-9999, inplace=True)\n","selection_c_copy.loc[0:10]"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"eWKn45Yz0HYS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As a result we now have a DataFrame where **NoData** values are filled with the value -9999. I have demonstrated both approaches with the same dataset, but in practise you would most likely chose one approach or the other!"],"metadata":{"deletable":true,"editable":true,"id":"Q6azx4Br0HYT"}},{"cell_type":"markdown","source":["\n","---\n",">**Warning**\n",">\n",">In many cases filling the data with a specific value is dangerous because you end up modifying the actual data, which might affect the results of your analysis. For example, in the case above we could have dramatically changed calculations based on temperature. This is because the -9999 values are interpreted by the computer as an numerical value but do not represent an actual temperature! Hence, use caution when filling missing values.\n",">\n","\n","> **Note**   \n",">You might have to fill in no data values for the purposes of saving the data to file in a specific format. For example, some GIS software does not accept missing values. Always pay attention to potential no data values when reading in data files and doing further analysis!\n","---"],"metadata":{"deletable":true,"editable":true,"id":"37TNPBc40HYT"}},{"cell_type":"markdown","source":["## Data type conversions"],"metadata":{"id":"vpRVEJJD0HYT"}},{"cell_type":"markdown","source":["There are occasions where you'll need to convert data stored within a Series to another data type, for example, from floating point to integer."],"metadata":{"editable":true,"id":"wRAtcMVV0HYT"}},{"cell_type":"markdown","source":["Remember, that we already did data type conversions using the [built-in Python functions](https://docs.python.org/3/library/functions.html#built-in-functions) such as `int()` or `str()`."],"metadata":{"id":"oCjuKL4A0HYU"}},{"cell_type":"markdown","source":["For values in pandas DataFrames and Series, we can use the `astype()` method."],"metadata":{"id":"gmsQQjw70HYU"}},{"cell_type":"markdown","source":["---\n",">**Truncating versus rounding up**\n",">\n",">**Be careful with type conversions from floating point values to integers.** The conversion simply drops the stuff to the right of the decimal point, so all values are rounded down to the nearest whole number. For example, 99.99 will be truncated to 99 as an integer, when it should be rounded up to 100.\n",">\n",">Chaining the round and type conversion functions solves this issue as the `.round(0).astype(int)` command first rounds the values with zero decimals and then converts those values into integers.\n","---"],"metadata":{"editable":true,"id":"JQvAjA8-0HYU"}},{"cell_type":"code","source":["print(\"Original values:\")\n","selection_c['min_air_temp'].head()"],"metadata":{"id":"tYK-Ug9q0HYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Truncated integer values:\")\n","selection_c[\"min_air_temp\"].astype(int).head()"],"metadata":{"id":"m75qX_hP0HYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Rounded integer values:\")\n","selection_c[\"min_air_temp\"].round(0).astype(int).head()"],"metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"0xxoDpq90HYV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looks correct now."],"metadata":{"id":"iZxClUpD0HYV"}},{"cell_type":"markdown","source":["## Unique values"],"metadata":{"deletable":true,"editable":true,"id":"BJvJxdVm0HYV"}},{"cell_type":"markdown","source":["Sometimes it is useful to extract the unique values that you have in your column.\n","We can do that by using `unique()` method:"],"metadata":{"deletable":true,"editable":true,"id":"lNBAEcQp0HYV"}},{"cell_type":"code","source":["# Get unique max_air_temp values\n","unique = selection_c[\"max_air_temp\"].unique()\n","unique"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"0XjfV94Q0HYV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As a result we get an array of unique values in that column."],"metadata":{"deletable":true,"editable":true,"id":"5ntkfcZj0HYV"}},{"cell_type":"markdown","source":[">**Note**\n",">\n",">Sometimes if you have a long list of unique values, you don't necessarily see all the unique values directly as IPython/Jupyter may hide them with an ellipsis `...`. It is, however, possible to see all those values by printing them as a list\n"],"metadata":{"deletable":true,"editable":true,"id":"QMtoUf_w0HYV"}},{"cell_type":"code","source":["# unique values as list\n","list(unique)"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"NcW2tOt60HYW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nZDsUIgWxIuY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[link text](https://)How many days with unique max_air_temp temperature did we have in 2020? We can check that!\n"],"metadata":{"deletable":true,"editable":true,"id":"L8tHaoQ30HYW"}},{"cell_type":"code","source":["unique_temps = len(unique)\n","print(f\"There were {unique_temps} days with unique max_air_temps temperatures in 2020.\")"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"g6bGybWC0HYW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sorting data\n","\n","Quite often it is useful to be able to sort your data (descending/ascending) based on values in some column\n","This can be easily done with pandas using the `sort_values(by='YourColumnName')` function.\n","\n","Let's first sort the values on ascending order based on the `TEMP` column:"],"metadata":{"id":"enh-w4e00HYW"}},{"cell_type":"code","source":["# Sort dataframe, ascending\n","selection_c.sort_values(by=\"max_air_temp\")"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"qTX-JQXn0HYW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of course, it is also possible to sort them in descending order with ``ascending=False`` parameter:\n"],"metadata":{"deletable":true,"editable":true,"id":"iwXBBuca0HYW"}},{"cell_type":"code","source":["# Sort dataframe, descending\n","selection_c.sort_values(by=\"max_air_temp\", ascending=False)"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"-UEdiP-O0HYX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Writing data to a file\n","\n","Lastly, it is of course important to be able to write the data that you have analyzed into your computer. This is really handy in pandas as it [supports many different data formats by default](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).\n","\n","**The most typical output format by far is a CSV file.** The function `to_csv()` can be used to easily save your data in CSV format. Let's first save the data from our `data` DataFrame into a file called `SY_temps_2020.csv`."],"metadata":{"deletable":true,"editable":true,"id":"_awo6Myb0HYX"}},{"cell_type":"code","source":["# define output path and filename\n","output_fp = \"/content/drive/MyDrive/Colab Notebooks/SY_temps_2020.csv\"\n","\n","# Save dataframe to csv\n","data.to_csv(output_fp, sep=\",\")"],"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"LI1gNZQk0HYX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we have the data from our DataFrame saved to a file.\n","\n","Open the file in Excel and you should see the first columns in the datafile now contains the index value of the rows.\n","Omitting the index can be done with the `index=False` parameter. Specifying how many decimals should be written can be done with the float_format parameter where the text %.1f instructs pandas to use 1 decimal in all columns when writing the data to a file (changing the value 1 to 2 would write 2 decimals, etc.)\n","\n"],"metadata":{"deletable":true,"editable":true,"id":"ehs_Ubt80HYX"}},{"cell_type":"code","source":["\n","selection_c.to_csv(\"/content/drive/MyDrive/Colab Notebooks/SY_temps_2020_clean.csv\", float_format='%.1f') #this only changes the TEMP_F column as all the others are already only to 1 decimal place."],"metadata":{"id":"dEXUuV_qitXy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That's it for this week. We will dive deeper into data analysis with pandas in the following Lesson."],"metadata":{"id":"kxJQUJnL0HYY"}}]}