{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1drRICYuew5ttcc1V0_61LC1zbgECSBz-","timestamp":1713356388536}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# STATS19 Workshop 1\n","\n","In this lesson, we are going to start looking at the datasets you will use for the assessment.\n","In particular, we will look at **joining** datasets together. We will also explore further plotting options using the pandas and matplotlib libraries.\n","\n","---\n","\n","##Data\n","\n","The data we will be working with from now on is **Road Safety Data**. It is collected by the [Department for Transport](https://www.data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data). We have extracted a subset of this data for South Yorkshire using the [STATS19 R package](https://CRAN.R-project.org/package=stats19) developed by Robin Lovelace et al*.\n","\n","The data comprises of three data files and 1 metadata file:\n","\n","* crashes_sy_201_2020.csv\n","* vehicles_sy_2016_2020.csv\n","* casulaties_sy_2016_2020.csv\n","* Road-Safety-Open-Dataset-Data-Guide.xlsx (metadata)\n","\n","\\* Lovelace R, Morgan M, Hama L, Padgham M, Ranzolin D, Sparks A (2019). “stats 19: A package for working with open road crash data.” The Journal of Open Source Software, 4(33), 1181. doi:10.21105/joss.01181.\n","\n","> **A note about terminology**. Many of the official datasets refer to road or traffic *accidents*. However, some campaigning groups object to this label on the grounds that it obscures the preventable nature of these events. The [#crashnotaccident](https://www.roadpeace.org/get-involved/crash-not-accident/) movement prefers the use of the term *crash* when road traffic incidents are described.\n","\n","---\n","\n","We will start by looking at the `crashes_sy_201_2020.csv` dataset.\n","\n"],"metadata":{"id":"i4tLMDr2l5Yq"}},{"cell_type":"markdown","source":["First, mount your Google drive"],"metadata":{"id":"scydsD1-rWF1"}},{"cell_type":"code","source":["#mount your Google drive"],"metadata":{"id":"rm9uDf8wrb3L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import the following libraries:\n","* `pandas` as `pd`\n","* `numpy` as `np`\n","* and `matplotlib.pyplot` as `plt`"],"metadata":{"id":"nQrrsSgBreo5"}},{"cell_type":"code","source":["#import the required libraries\n","\n","\n"],"metadata":{"id":"DzkEsHaDr0YK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, read in the  `crashes_sy_201_2020.csv` dataset. You can do this however you wish, but I recommend creating a 'path variable' as we have done in previous workshops."],"metadata":{"id":"_xyA-31pXMtN"}},{"cell_type":"code","source":["#create a file path variable for the  crashes_sy_201_2020_v2.csv dataset\n"],"metadata":{"id":"myCqcTk65WuL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#read in the  crashes_sy_2016_2020_v2.csv dataset.\n"],"metadata":{"id":"2P-Ru5cnFN1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Click here for the code\n","#create a file path variable for the  crashes_sy_201_2020.csv dataset\n","fp= r'/content/drive/Shareddrives/TRP479_Spatial_Data_Science_Data/Assessment_Data/crashes_sy_2016_2020_v2.csv'\n","df=pd.read_csv(fp)"],"metadata":{"id":"q_GCZSKZFKgx","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check that the file has read in correctly. It should have a header row with attribute (column) names and 12714 rows and 38 columns.\n","\n"],"metadata":{"id":"VZyaeoTdFUso"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is quite a large dataset so it may be easier to explore with an interactive table.\n","\n","---\n","> ### Note\n",">\n",">By default, Colab interactive tables are limited to 20 columns. Our dataset has 38; in order to view *all* of the columns in our data, we need to change the default oprions. We an do that by running the code:\n",">\n",">```\n",">from google.colab.data_table import DataTable\n","DataTable.max_columns = 40\n",">```\n",">\n",">(This answer came from a quick internet search on the search terms 'google colab interactive tables with many columns'; one of the first results was [this one from Stack Overflow](https://stackoverflow.com/questions/68588977/google-colaboratory-data-table-display-max-20-columns))\n","\n","\n","\n","---\n","\n","\n","\n","\n"],"metadata":{"id":"S76K-hYZYZJT"}},{"cell_type":"code","source":["# increase the DataTable.max_columns setting to 50\n","\n"],"metadata":{"id":"FtnodZd35nMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#reload the Datatable for the crashes data\n"],"metadata":{"id":"55YcaPoZ54mt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the interactive table, answer these questions in the text cell below:\n","\n","\n","1.   Which attributes contain *spatial* information?\n","2.   Which attributes contain *temporal* information?\n","3.  Which attributes contain variables that might be important to understanding road traffic accidents? State if these are *quanitative* or *categorical* variables."],"metadata":{"id":"0bldyYnM51E_"}},{"cell_type":"markdown","source":["\n","### Your answers:\n","1.   \n","2.  \n","3.\n","\n"],"metadata":{"id":"XkHr1BiHaIma"}},{"cell_type":"markdown","source":["You should have identifed a couple of variables for question 3 that might be interesting to explore further. We will come back to these later.\n","\n","##Data exploration with pandas\n","\n","One of the first things we might want to know is the *distribution* of our chosen variable. If it is a **numerical** attribute, we can look at descriptive statistics like the *mean*, *median* and *range*, for example. For **categorical** variables, we can only use the *mode* as a descriptive statistic. We can, however, look at the *frequency* of different observations.\n","In python, we can use the `value_counts()` method to a count of the unique values in a column.\n","\n","For example, `df[\"local_authority_district\"].value_counts()` will return a series with the number of times each Local Authority is recorded in the \"local_authority_district\" column.\n","\n","  > Note that the `values_counts()` method will identify all possible unique values *and* calculate the number of times each occurs. They are also ordered from most frequent to least frequent.\n","\n","Let's start by using `.describe()` method we used in Lesson 5 to get some general descriptive statistics."],"metadata":{"id":"3CLW_IxUcCJ_"}},{"cell_type":"code","source":["#generate basic descriptive statistics using the .describe() method [see Lesson 5 notebook: TRP479_5_1_exploring-data-using-pandas.ipynb]\n"],"metadata":{"id":"8cOgxbqhsX0a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","**Question** [edit this cell with your answers]\n","\n","Even for numerical variables, calculating descriptive statistics is not always meaningful.\n","\n","List the variables where the *descriptive* statistics **are** meaningful:\n","\n","*   *List item*\n","*   *List item*\n","\n","What can you tell about these variables from the descriptive statistics?\n","\n","\n","\n","\n","\n","---"],"metadata":{"id":"p5HIjZ10tiVo"}},{"cell_type":"markdown","source":["Now, lets have a closer look at the frequency and distribution of our data using the `value_counts()` method. We are going to start by looking at a *categorical* variable, \"local_authority_district\"."],"metadata":{"id":"N5abgBFpu3M5"}},{"cell_type":"code","source":["#calcuate the frequency of accidents in different local_authorities\n","df[\"local_authority_district\"].value_counts()\n"],"metadata":{"id":"0tt192hYICCv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By default, `value_counts()` will *sort* the output in descending order. You can disable sorting by using the parameter `sort=False`.\n","\n","> **Note** without *sorting*, categories will be ordered by the order in which they appear in the data.\n","\n"],"metadata":{"id":"_4M8AcM-fGer"}},{"cell_type":"code","source":["#calcuate the frequency of accidents in different local_authorities by order of appearance in the data (`sort=False`)\n","df[\"local_authority_district\"].value_counts(sort=False)"],"metadata":{"id":"y9F_W5ESfc3D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It is often more helpful to look at *relative frequences*, i.e. the proportion of the toal in each category. We can do that by using the `normalize=True` parameter with `value_counts()`."],"metadata":{"id":"1xxYwxeIfbiB"}},{"cell_type":"code","source":["df[\"local_authority_district\"].value_counts(sort=False,normalize=True)"],"metadata":{"id":"kffrBnUZhZwF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `value_counts()` method will also work with *numerical* data. Let's have a look at the number of vehicles involved in each crash in South Yorkshire."],"metadata":{"id":"oc88DN-8hopK"}},{"cell_type":"code","source":["df[\"number_of_vehicles\"].value_counts()"],"metadata":{"id":"RwSwK1C4vbzr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this case, the first column lists the unique values in the `number_of_vehicles` column and the second column tallies up how many times each value appears. This shows us that almost all vehicles involve only 1 or 2 vehicles (3451 and 7864 crashes, respectivly).\n","\n","In the code cell below, write a line of code that will allow you to work out what *proportion* of crashes involve 2 vehicles [**tip**: look at what we have just been doing]."],"metadata":{"id":"X4GfhnIywEPG"}},{"cell_type":"code","source":["#what is the proportion of crashes that involved 2 vehicles?\n"],"metadata":{"id":"41UA8_BowZik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For *numerical* variables, we can also use `value_counts()` to group our data into *bins* using the `bins` parameter. This can be helpful if there are a large number of unqiue values."],"metadata":{"id":"-wRvmdbYw4Jz"}},{"cell_type":"code","source":["#calculate the distribution of values into 5 bins, normalised to show proportions of the total.\n","df[\"number_of_vehicles\"].value_counts(bins=5,normalize = True)"],"metadata":{"id":"qUwdpatNxLdz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Question**\n","\n","What does this tell us about how many vehicles are typically involved in crashes?\n","\n","\n","* *Write your anser here*\n","---"],"metadata":{"id":"sUg0hiW-xUg0"}},{"cell_type":"markdown","source":["## Converting to time/date information\n","\n","As with the data we worked with in Lesson 6, we can see several variables with useful time and date information. However, these are not imported in a time/date format. To make it easier to work with the data, let's convert it to a 'timedate' format.\n","\n","We can use the `astype()` function to convert it to a `datetime64[ns]` format.\n","\n","> `datetime64[ns]` is a type defined by the `numpy` package to hold date/time information."],"metadata":{"id":"k63BJaS7HOLR"}},{"cell_type":"code","source":["#convert the 'date' column and 'time' column from string objects to datetime64[ns] objects\n","df['DATE'] = df['date'].astype('datetime64[ns]')\n","df['TIME']=df['time'].astype('datetime64[ns]')"],"metadata":{"id":"BVQtlalwI35P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These two lines create new collumns (`DATE` and `TIME`) which contain the 'date' and 'time' as 'datetime' objects (you could check this by checing the datatypes). The benefit of using 'datetime' objects is that we can easily extract elements of dates and times; for example, we can use the pandas `DatetimeIndex` structure, from which we can than access attributes like  `month` and `hour`.\n","\n","> The [DatetimeIndex](https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html)  structure has lots of different attributes to work with, explore the linked page to find out what other options there are."],"metadata":{"id":"sjapqK86OfHC"}},{"cell_type":"code","source":["#create a new variable 'MONTH' which contains the month of the each accident.\n","df['MONTH'] = pd.DatetimeIndex(df['DATE']).month\n"],"metadata":{"id":"g7t-HSS1K2To"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## Question\n","\n","Which months have the most crashes?\n","\n"],"metadata":{"id":"6rbVwKcfSZdq"}},{"cell_type":"code","source":["#use this cell to work out which months have the most crashes in our data"],"metadata":{"id":"SMMz79I-LwYf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Following the format from the previous cell, create a new variable `HOUR` which containes the *hour* of the day in which each crash occurs."],"metadata":{"id":"OIJcK88QS2RP"}},{"cell_type":"code","source":["#complete this cell to create a new variable 'HOUR' which contains the 'hour' of each crash.\n","df['HOUR'] ="],"metadata":{"id":"S1drqSlVS6jS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is all very useful, but we would really like to be able to *visualise* the data we are working with. This would allow us to start to pick out patterns for further exloration or analysis.  "],"metadata":{"id":"OGLzRpDLGmo5"}},{"cell_type":"markdown","source":["---\n","\n","## Some basic plotting\n","\n","In lesson 5, we looked at some basic plotting using the `.plot()` method."],"metadata":{"id":"X4Btup6kxlr3"}},{"cell_type":"code","source":["#plot the 'MONTH' attribute\n","df['MONTH'].plot()"],"metadata":{"id":"eQxOhM3GZQ95"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This simply plots the month of each individual record and is not really very insightful; it is is hard to tell if there are any clear *seasonal* patterns, for example. Instead, we might want to *group* our data into months. We can combine the `values_counts()` method with the `plot()` method to create a plot of the frequency of crashes in each month."],"metadata":{"id":"vmMffvDoaIXb"}},{"cell_type":"code","source":["df[\"MONTH\"].value_counts().plot()"],"metadata":{"id":"BCM3F9H8ZmCD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is still not very helpful; for this type of data, it would be more appropriate to use a *bar* chart. We can change this easily by adding the `kind= 'bar'` parameter to our plot."],"metadata":{"id":"vAR-TDBNaxKw"}},{"cell_type":"code","source":["df[\"MONTH\"].value_counts().plot(kind='bar')"],"metadata":{"id":"EQdX2WBNbDDg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is a bit better but there are still some problems with this plot.\n","\n","**Question:** What is the problem with this plot? What could we add to the code to improve it?\n",""],"metadata":{"id":"YEKq5WfXbQau"}},{"cell_type":"code","source":["#@title Clear here to see a more sensible plot\n","df[\"MONTH\"].value_counts(sort=False).plot(kind='bar')\n"],"metadata":{"id":"XsQL3bUwF9VA","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now try making some bar plots of some of the other variables."],"metadata":{"id":"1Ak9VtaubsHh"}},{"cell_type":"code","source":[],"metadata":{"id":"3KtFS3f7dbes"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"om6JU-tHQf1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lt_57qvboTGs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## Plotting\n","\n","We can also plot data in combination with the `groupby` method in pandas. The following line of code will:\n","*  group the dataset by the `year` and then `month` attribute within the `['DATE']` column of our dataset (remember, this is the one we converted to a datetime format),\n","* then `count` the observations in each group.\n","* `plot` the output as a bar chart.\n","\n","```\n","df['DATE'].groupby([df[\"DATE\"].dt.year, df[\"DATE\"].dt.month]).count().plot(kind=\"bar\")\n","```\n"],"metadata":{"id":"qqk6eiWBegIl"}},{"cell_type":"code","source":["df['DATE'].groupby([df[\"DATE\"].dt.year, df[\"DATE\"].dt.month]).count().plot(kind=\"bar\")"],"metadata":{"id":"_E9LWByCfI0S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The x-axis is a little hard to read, so let's change the size of the figure. We can do that individually for a figure using the `figsize` parameter."],"metadata":{"id":"6TZt9XnGgqkT"}},{"cell_type":"code","source":["df['DATE'].groupby([df[\"DATE\"].dt.year, df[\"DATE\"].dt.month]).count().plot(kind=\"bar\",figsize=(10, 5))"],"metadata":{"id":"hY6gj7VIg1W9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","**Question**\n","\n","What patterns can you see from this visualisation?\n","\n","*\n","*\n","*\n","\n","---"],"metadata":{"id":"nJkphEH-f7Kl"}},{"cell_type":"markdown","source":["We can have more control over plotting using the `matplotlib` library, specifically the `pyplot` module. By convention we import is as shown:\n","\n","`import matplotlib.pyplot as plt`\n","\n","Plots in 'matplotlib' are housed within a `figure` object which can be created with the command:\n","\n","`fig=plt.figure()`\n","\n","> Unlike some other notebooks, Colab does not allow interactive `matplotlib` plotting so we can only use the `inline` mode which creates *static* images embedded in the notebook. [**Tip**: in a Jupyter notebook, for example, you can access interactive mode by executing the command ` %matplotlib notebook`].  "],"metadata":{"id":"R4CbDttldf9a"}},{"cell_type":"code","source":["#specify 'inline' mode\n","%matplotlib inline"],"metadata":{"id":"QTbv4i3ToO3t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will start by using the `hist()` function from `pyplot`. This automatically 'groups' our data into 10 bins."],"metadata":{"id":"JvWvoSB2pf4_"}},{"cell_type":"code","source":["\n","#create a matplotlib figure of the frequency of observations by month\n","plt.hist(df['MONTH'])\n"],"metadata":{"id":"tsNeD8wfgJfF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","**Question**\n","Why does this look different to the previous plot?\n","\n","---\n"],"metadata":{"id":"PdQ_xMwKpvyP"}},{"cell_type":"markdown","source":["\n","To change the number of bins, we can set the `bins` paramter."],"metadata":{"id":"ZaRM8ysjH0HH"}},{"cell_type":"code","source":["#create a matplotlib figure of the frequency of observations by month with 12 bins (one for each month)\n","plt.hist(df['MONTH'], bins =12)"],"metadata":{"id":"V4Zzw2B-p0HV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also specify the bins explicitly using the `bins` paramter to `histogram`.\n","\n","> **Note** the `numpy` package gives the following guidance on the 'bins' parameter:\n",">\n",">* If bins is an integer, it defines the number of equal-width bins in the range.\n",">*If bins is a sequence, it defines the bin edges, including the left edge of the first bin and the right edge of the last bin; in this case, bins may be unequally spaced. All but the last (righthand-most) bin is half-open. In other words:\n"," * if `bins =[1, 2, 3, 4]`\n","  * then the **first** bin is `[1, 2)` (including 1, but excluding 2) and\n","  * the **second** `[2, 3)` (including 2, but excluding 3).\n","  * The **last** bin, however, is `[3, 4]`, which includes 3 **and** includes 4."],"metadata":{"id":"QX-PXi8WKdXo"}},{"cell_type":"code","source":["#create a matplotlib figure of the frequency of observations by month with 12 bins (one for each month)\n","#note: due to how 'bin's are defined, the last bin here is 13.\n","plt.hist(df['MONTH'], bins =[1,2,3,4,5,6,7,8,9,10,11,12,13])"],"metadata":{"id":"DuCDWNiPH4NM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also start to experiment with visual styling. For example, if we want to separate the bins a little bit, we can define an edge colour for the bars."],"metadata":{"id":"pNr3rhOJMc8-"}},{"cell_type":"code","source":["plt.hist(df['MONTH'], bins =[1,2,3,4,5,6,7,8,9,10,11,12,13], edgecolor='white',linewidth=2)\n"],"metadata":{"id":"J0osrvFYMT9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","**Task:**\n","\n","Now you have a go at creating a figure of the frequency of observations by *hour* of the day; make sure you have 1 bin for each our of the day.\n","Use the parameters from the previous code box to put a thin, black line around each bar."],"metadata":{"id":"CPlbKgzYNcEA"}},{"cell_type":"code","source":["#create a matplotlib figure of the frequency of observations by HOUR\n","\n","\n"],"metadata":{"id":"QiHjrgk0NnPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Click here to show one solution\n","#create a matplotlib figure of the frequency of observations by HOUR\n","plt.hist(df['HOUR'],bins=24,edgecolor='black',linewidth=1)"],"metadata":{"cellView":"form","id":"VKtSxSYgqMjW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What does this tell us about the time of day that crashes are most likely to happen?\n","\n","---"],"metadata":{"id":"-EsEpBQgUljc"}},{"cell_type":"markdown","source":["It might also be interesting to look at crashes by **day** of the week. This is slightly trickier, this is becasue our `day_of_week` variable contains the day *name*.\n","\n","Have a go at visualising this data using the 'day_of_week' variable. Then think about other ways you could try and extract this information (**tip:"],"metadata":{"id":"gCW5vTyVUaVA"}},{"cell_type":"code","source":["#create a matplotlib figure of the frequency of observations by day of the week\n","\n","\n","\n"],"metadata":{"id":"I7zgXMtB-hm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Click to show one solution\n","#create a matplotlib figure of the frequency of observations by day of the week\n","\n","\n","df['DAY'] = pd.DatetimeIndex(df['DATE']).day_of_week\n","plt.hist(df['DAY'], bins =7, edgecolor='white',linewidth=2)"],"metadata":{"id":"G3tqIS59V-rv","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Joining data from one DataFrame to another\n","One quite useful functionality in Pandas is the ability to conduct a table join where data from one DataFrame is merged with another DataFrame based on a common key. Hence, making a table join requires that you have at least one common variable in both of the DataFrames that can be used to combine the data together.\n","\n","Consider a following example. Let’s first create some test data to our DataFrames.\n","```\n","data1 = pd.DataFrame(data=[['20170101', 'Pluto'], ['20170102', 'Panda'], ['20170103', 'Snoopy']], columns=['Time', 'Favourite_dog'])\n","\n","data2 = pd.DataFrame(data=[['20170101', 1], ['20170101', 2], ['20170102', 3], ['20170104', 3], ['20170104', 8]], columns=['Time', 'Value'])\n","\n","data1\n","Out[3]:\n","       Time Favourite_dog\n","0  20170101         Pluto\n","1  20170102         Panda\n","2  20170103        Snoopy\n","\n","data2\n","Out[4]:\n","       Time  Value\n","0  20170101      1\n","1  20170101      2\n","2  20170102      3\n","3  20170104      3\n","4  20170104      8\n","\n","```\n","\n","As we can see here, there different number of rows in the DataFrames. Important thing to notice is that there seems to be a common column called `Time` that we can use to join these DataFrames together. In Pandas, we can conduct a table join with `merge` function. Consider following example where we join the data from `data2` DataFrame to `data1` DataFrame.\n","\n","```\n","join1 = data1.merge(data2, on='Time')\n","\n","join1\n","Out[6]:\n","       Time Favourite_dog  Value\n","0  20170101         Pluto      1\n","1  20170101         Pluto      2\n","2  20170102         Panda      3\n","```\n","\n","Ahaa! Now we can see that we managed to get the `Value` column from `data2` in our `data1` DataFrame (here we just assigned those values to a new variable `join1`).\n","\n","Notice that 'Pluto' apears twice in the joined DataFrame although it only occurred once in the original one. Pandas automatically duplicates the values in such columns where there are more matching values in one DataFrame compared to the other. In other words it has joined *all* the values from the data2 that match the key value (in this case 2 entries for `Time` == `20170101`).\n","\n","However, it is important to notice that there were more values in the `data2` DataFrame than in `data1`. The result, `join1`, does not contain the values 3 and 8 that were from day `20170104` and they were omitted.\n","\n","This might be OK, but in some cases it is useful to also bring *all* values from another DataFrame - even though there would not be a matching value in the column that used for making the join (i.e. the key).\n","\n","We can bring all the values from another DataFrame by specifying parameter `how='outer'`, i.e. we will make an *outer* join. Let’s consider another example with the outer join.\n","```\n","join2 = data1.merge(data2, on='Time', how='outer')\n","\n","join2\n","Out[8]:\n","       Time Favourite_dog  Value\n","0  20170101         Pluto    1.0\n","1  20170101         Pluto    2.0\n","2  20170102         Panda    3.0\n","3  20170103        Snoopy    NaN\n","4  20170104           NaN    3.0\n","5  20170104           NaN    8.0\n","\n","```\n","Cool! Nowe we have all the values included from both DataFrames and if Pandas did not find a common value in the key column, it still kept them and inserted NaN values into Favourite_dog column and Value column.\n","\n","Overall, knowing how to conduct a table join can be really handy in many different situations. See more examples about joinging data with [merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) from the official documentation of Pandas."],"metadata":{"id":"zAFBmJdClY6L"}},{"cell_type":"markdown","source":["This was a very trivial example, let's have a look at our datasets.\n","\n","First, I want you to use your skills to read in the **'vehicles_sy_2016_2020_v2.csv'** dataset, then spend a little bit of time exploring it.\n"],"metadata":{"id":"B5zhCjPpk1Lp"}},{"cell_type":"code","source":["#read in the 'vehicles_sy_2016_2020_v2.csv' file\n","# then check it looks right\n","#then spend some time explroring the data\n","\n"],"metadata":{"id":"ehHzPVmXlEp4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once you have done that, see if you can dentify the common identifier or key between the 'vehicles' dataset and the 'crashes' dataset we were using at first.\n","\n","We are going to *join* the two datasets together using the `accident_index` attribute - this is the *common identifier*.\n","\n","> **Note** in the `vehicles` dataset, you will notice that more than one vehicle may have the same `accident_index`; this is because more than one vehicle may have been involved in a single crash incident.\n","\n","We are going to join the *crashes* data to the *vehicles* data using a *left* join."],"metadata":{"id":"LxuOJAuel1uf"}},{"cell_type":"code","source":["#use a 'left' join to merge the crahses data set to the vehicles dataset\n","join_v = df2.merge(df, on='accident_index', how='left')"],"metadata":{"id":"bxneWoYamwE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check your joined data\n","join_v.head()"],"metadata":{"id":"BXhecdXTm95f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's use this *joined* data to see if we can see any difference in the crashes involving different *types* of vehicles.\n","\n","First, lets have a look at the different vehicles listed in the `vehicle_types` attribute:"],"metadata":{"id":"LZA3tmQ8-Ugx"}},{"cell_type":"code","source":["#use the 'value_counts()' method to look at the different possible categories in the vehicle_type attribute\n","join_v['vehicle_type'].value_counts()"],"metadata":{"id":"J68dOYHWoTlw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a histogram of the frequency of different vehicles\n","\n"],"metadata":{"id":"qrqMHcy_-p8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a histogram of the frequency of different vehicles\n","\n","join_v['vehicle_type'].value_counts().plot(kind='bar')\n"],"metadata":{"id":"y-I1fjOt_Hb-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This gives us some information about our data - i.e most accidents involve private cars!\n","\n","But beyond that, we can't tell very much. It might be interesting to see if there are *temporal* differences. We can do this by looking at *slices* of our data. In the code below:\n","\n","* the first line creates a variable called `vtype`; to start with, I have fileed it with the string \"Car\" but I can easily change this if I want to look at other vehicle types (e.g. \"Tram\" or \"Motorcycle over 500cc\").\n","\n","* The second line, creates a *slice* of the joined data set with only those rows where the `vehicle_type` value is the same as my variable `vtype` (\"Car\" in the code beow).\n","\n","* The third line creates a pyplot histogram of the frequency of different 'hours'.\n","\n","* The final line gives the plot a title based on the variable `vtype`.\n","\n","We can use this bit of code to easily look at individual vehicly types but there are 21 different vehile type categories so it would be useful to do this more efficiently...\n"],"metadata":{"id":"Dk2Mo1n6A0Bc"}},{"cell_type":"code","source":["vtype=\"Car\"\n","data = join_v[join_v.vehicle_type == vtype]\n","plt.hist(data['HOUR'], bins =24, edgecolor='white')\n","plt.title(vtype)"],"metadata":{"id":"BoZEeRwNIwnK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use *loops* to cycle through our different categories to create a panel plot for each vehicle type."],"metadata":{"id":"DZuVTJ5pK_kg"}},{"cell_type":"code","source":["fig, axes=plt.subplots(nrows=7, ncols=3) #sets up the subplots; we know there are 21 categories so we will can use 7 rows of 3\n","\n","\n","for i, vehicle in enumerate(join_v.vehicle_type.unique(), 1):  # iterate through each unique vehicle_type\n","    data = join_v[join_v.vehicle_type == vehicle]  # filter by vehicle type\n","    plt.subplot(7, 3, i)  # rows, columns, i: plot index beginning at 1\n","    plt.hist(data['HOUR'],  edgecolor='white')\n","    plt.title(vehicle) #puts a title on each subplot\n"],"metadata":{"id":"Ga4Ksba_K-ZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is quite hard to read; the first thing we might want to do is make the figure bigger. We can do that in the first line when we set up the subplots by adding the `figsize` parameter\n","```\n","fig, axes=plt.subplots(nrows=7, ncols=3,figsize=(15,25))\n","```\n","> **Note**  matplotlib uses inches as units by default so `figsize=(15,25))` will create a plot 15 inches wide and 25 inches in height).\n","\n","We can also adsome padding between our plots so that axis labels and titles don't overlap.\n","```\n","plt.subplots_adjust(hspace=0.5)\n","```"],"metadata":{"id":"c4AWIqg7MfsO"}},{"cell_type":"code","source":["fig, axes=plt.subplots(nrows=7, ncols=3,figsize=(15,25))\n","plt.subplots_adjust(hspace=0.5)\n","for i, vehicle in enumerate(join_v.vehicle_type.unique(), 1):  # iterate through each unique vehicle_type\n","    data = join_v[join_v.vehicle_type == vehicle]  # filter by vehicle type\n","    plt.subplot(7, 3, i)  # rows, columns, i: plot index beginning at 1\n","    plt.hist(data['HOUR'], edgecolor='white')\n","    plt.title(vehicle)"],"metadata":{"id":"7X0I3vX-MUvL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This looks better, but if you look closely, you may see that the x axis are not all the same... This makes it hard to draw accurate comparisons; we also know that there are 24 hours in the day, so it makes sense to have 24 bins.\n","\n","We can do that by using the [`range'](https://www.w3schools.com/python/ref_func_range.asp) function we used early on in the module.\n","\n","```\n","plt.hist(data['HOUR'], bins =range (0,25), edgecolor='white')\n","```\n","> **Note** remember that the `range` function is *inclusive* of the first number but *exclusive* of the last number."],"metadata":{"id":"Pv38-uadN-e4"}},{"cell_type":"code","source":["fig, axes=plt.subplots(nrows=7, ncols=3,figsize=(15,25))\n","plt.subplots_adjust(hspace=0.5)\n","for i, vehicle in enumerate(join_v.vehicle_type.unique(), 1):  # iterate through each unique vehicle_type\n","    data = join_v[join_v.vehicle_type == vehicle]  # filter by vehicle type\n","    plt.subplot(7, 3, i)  # rows, columns, i: plot index beginning at 1\n","    plt.hist(data['HOUR'], bins =range (0,25), edgecolor='white')\n","    plt.title(vehicle)\n","\n","    #  this was a useful page for plotting subplots  https://engineeringfordatascience.com/posts/matplotlib_subplots/"],"metadata":{"id":"w436qpvwNj5U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This allows to get a quick picture of the `vehicle_type` variable may vary by time of day. Can you pick out any distinctive pattens? What might be driving them?\n","\n","\n","*\n","\n","*\n","\n","*\n","\n","There are other ways of creating supblots and lots of ways you can visualise and analyse this data. Think of other variables that might be interesting to visualise and have a go at grouping and slicing the data in different ways"],"metadata":{"id":"FYYTsO84QHbs"}}]}